{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7319031,"sourceType":"datasetVersion","datasetId":4212640}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:12:45.577067Z","iopub.execute_input":"2024-01-05T14:12:45.577396Z","iopub.status.idle":"2024-01-05T14:12:58.298876Z","shell.execute_reply.started":"2024-01-05T14:12:45.577370Z","shell.execute_reply":"2024-01-05T14:12:58.297780Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Obtaining dependency information for pycocotools from https://files.pythonhosted.org/packages/ba/64/0451cf41a00fd5ac4501de4ea0e395b7d909e09d665e56890b5d3809ae26/pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.24.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np                                    # Importa Numpy\nimport skimage.io as io                               # Importa il modulo Input/ouput di SK-Image\nfrom skimage.transform import resize                  # Importa il modulo resize da SK-Image\nfrom os import listdir                                # Importa il modulo listdir da OS\n\nimport json                                           # Importa Json\nfrom matplotlib.collections import PatchCollection    # Importa PatchCollection dal modulo collections di MatPlotLib\nfrom pycocotools.coco import COCO                     # Importa COCO dal modulo coco di PyCoco-Tools\nimport pycocotools.mask as cocomask                   # Importa il modulo Mask di PyCoco-Tools\nimport matplotlib.pyplot as plt                       # Importa il modulo  pyplot di MatPlotLib\n\nimport PIL\nfrom PIL import Image, ImageDraw                      # Importa il modulo Image da PIL\n\nfrom tensorflow import keras                          # Importa il modulo Keras di TensorFlow\nimport os                                             # Importa os\n\nfrom tqdm import tqdm                                 # Importa il modulo tqdm da tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.transforms import transforms\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\nfrom torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\nfrom torchvision.models.detection.roi_heads import RoIHeads\nfrom torchvision.ops import MultiScaleRoIAlign\n\nfrom typing import Dict, List, Optional, Tuple\n\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:12:58.301370Z","iopub.execute_input":"2024-01-05T14:12:58.301783Z","iopub.status.idle":"2024-01-05T14:13:13.635338Z","shell.execute_reply.started":"2024-01-05T14:12:58.301742Z","shell.execute_reply":"2024-01-05T14:13:13.634535Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Path\npath_in = '/kaggle/input/dataset-ipcv-teama/'\npath_out = '/kaggle/working/'\n\ncoco_path = path_in + 'COCO_annotations.json/COCO_annotations.json'\ntrain_path = path_in + 'dataset_IPCV/kaggle/working/train'\ntest_path = path_in + 'dataset_IPCV/kaggle/working/test'\nval_path = path_in + 'dataset_IPCV/kaggle/working/val'\n\nweight_path = path_out + 'weight/'","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.636538Z","iopub.execute_input":"2024-01-05T14:13:13.637262Z","iopub.status.idle":"2024-01-05T14:13:13.642947Z","shell.execute_reply.started":"2024-01-05T14:13:13.637227Z","shell.execute_reply":"2024-01-05T14:13:13.641955Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Def. di Funzioni","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n    \"\"\"\n    Carica un'immagine da un percorso specificato.\n\n    Precondizione:\n        - `image_path` è il percorso completo dell'immagine da caricare.\n\n    Postcondizione:\n        - Restituisce l'immagine caricata.\n        - Solleva un'eccezione AssertionError se l'immagine non viene trovata nel percorso specificato.\n    \"\"\"\n    image = plt.imread(image_path)\n    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.645382Z","iopub.execute_input":"2024-01-05T14:13:13.645677Z","iopub.status.idle":"2024-01-05T14:13:13.685055Z","shell.execute_reply.started":"2024-01-05T14:13:13.645651Z","shell.execute_reply":"2024-01-05T14:13:13.684130Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class BuildingsDataset(Dataset):\n    def __init__(self, images_dir, annotation_path, transform=None):\n        \"\"\"\n        Inizializza un oggetto BuildingsDataset.\n\n        Precondizioni:\n            - `images_dir` è il percorso della directory contenente le immagini.\n            - `annotation_path` è il percorso del file JSON contenente le annotazioni.\n            - `transform` è un oggetto trasformazione (es. da torchvision.transforms) per applicare\n              trasformazioni alle immagini e alle etichette, opzionale.\n\n        Postcondizioni:\n            - `image_paths` è una lista di percorsi completi delle immagini nella directory `images_dir`.\n            - `annotations` contiene le annotazioni caricate dal file JSON specificato.\n            - `transform` è l'oggetto trasformazione fornito.\n        \"\"\"\n        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n        self.transform = transform\n \n        with open(annotation_path, 'r') as f:\n            self.annotations = json.load(f)\n \n    def transform_image_bbox(self, image, label):\n        \"\"\"\n        Applica trasformazioni all'immagine e alle etichette, se definite.\n\n        Precondizioni:\n            - `image` è l'immagine da trasformare.\n            - `label` sono le etichette associate all'immagine.\n\n        Postcondizioni:\n            - Se `transform` è definito, applica le trasformazioni all'immagine e alle etichette.\n            - Restituisce l'immagine e le etichette trasformate.\n        \"\"\"\n        if self.transform:\n            transformed = self.transform(image=image, labels=label)\n            image = transformed['image']\n            label = transformed['labels']\n \n        image = transforms.ToTensor()(image)\n        return image, label\n \n    def __getitem__(self, index):\n        \"\"\"\n        Restituisce un campione specifico dell'insieme di dati.\n\n        Precondizioni:\n            - `index` è l'indice del campione da restituire.\n\n        Postcondizioni:\n            - Restituisce un'immagine e le relative etichette nel formato richiesto.\n              Se non ci sono annotazioni per l'immagine, restituisce un target vuoto.\n        \"\"\"\n        image_path = self.image_paths[index]\n        image_name = image_path.split(\"/\")[-1]\n        image_id = next((img['id'] for img in self.annotations['images'] if img['file_name'] == image_name), None)\n        image = load_image(image_path).astype(np.float32)\n        image /= 255.0\n        image = torch.from_numpy(image).permute(2,0,1) # change the shape from [h,w,c] to [c,h,w]  \n\n        box_lab = [anno for anno in self.annotations['annotations'] if anno['image_id'] == image_id]\n\n        if(len(box_lab) == 0):\n            target = {}\n            boxes = torch.zeros((0, 4), dtype=torch.float32) \n            target = {\n                    \"boxes\": boxes, \n                    \"labels\": torch.zeros(0, dtype=torch.int64), \n                    \"image_id\": torch.as_tensor([4])\n                     }\n        else:\n            boxes = [json.loads(record['bbox']) for record in box_lab]\n            categories = [record['category_id'] for record in box_lab]\n            boxes = np.array(boxes)\n            # change the co-ordinates into expected [x, y, x+w, y+h] format\n            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n            labels = torch.as_tensor(categories, dtype=torch.int64)\n\n            target = {\n                   \"boxes\": boxes, \n                   \"labels\": labels,\n                   \"image_id\": torch.tensor([index])\n                    }\n                \n        return image, target\n \n    def __len__(self):\n        \"\"\"\n        Restituisce il numero totale di campioni nell'insieme di dati.\n\n        Postcondizioni:\n            - Restituisce la lunghezza dell'insieme di dati.\n        \"\"\"\n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.686644Z","iopub.execute_input":"2024-01-05T14:13:13.687004Z","iopub.status.idle":"2024-01-05T14:13:13.704193Z","shell.execute_reply.started":"2024-01-05T14:13:13.686972Z","shell.execute_reply":"2024-01-05T14:13:13.703322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ImageList:\n    \"\"\"\n    Structure that holds a list of images (of possibly\n    varying sizes) as a single tensor.\n    This works by padding the images to the same size,\n    and storing in a field the original sizes of each image\n\n    Args:\n        tensors (tensor): Tensor containing images.\n        image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.\n    \"\"\"\n\n    def __init__(self, tensors: Tensor, image_sizes: List[Tuple[int, int]]) -> None:\n        self.tensors = tensors\n        self.image_sizes = image_sizes\n\n    def to(self, device: torch.device) -> \"ImageList\":\n        cast_tensor = self.tensors.to(device)\n        return ImageList(cast_tensor, self.image_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.705305Z","iopub.execute_input":"2024-01-05T14:13:13.706153Z","iopub.status.idle":"2024-01-05T14:13:13.718352Z","shell.execute_reply.started":"2024-01-05T14:13:13.706119Z","shell.execute_reply":"2024-01-05T14:13:13.717582Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def _default_anchorgen():\n    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n    return AnchorGenerator(anchor_sizes, aspect_ratios)\n \n# Definisci la tua implementazione personalizzata della RPN\nclass CustomRegionProposalNetwork(RegionProposalNetwork):\n    def __init__(\n        self,\n        anchor_generator: AnchorGenerator,\n        head: nn.Module,\n        # Faster-RCNN Training\n        fg_iou_thresh: float,\n        bg_iou_thresh: float,\n        batch_size_per_image: int,\n        positive_fraction: float,\n        # Faster-RCNN Inference\n        pre_nms_top_n: Dict[str, int],\n        post_nms_top_n: Dict[str, int],\n        nms_thresh: float,\n        score_thresh: float = 0.0,\n    ):\n        # Implementa il tuo costruttore personalizzato\n        super().__init__(\n            anchor_generator,\n            head,\n            fg_iou_thresh,\n            bg_iou_thresh,\n            batch_size_per_image,\n            positive_fraction,\n            pre_nms_top_n,\n            post_nms_top_n,\n            nms_thresh,\n            score_thresh,\n        )\n        \n        # Inizializza la tua RPN personalizzata con i parametri necessari\n        # ...\n \n    def compute_loss(\n    self, objectness: Tensor, pred_bbox_deltas: Tensor, labels: List[Tensor], regression_targets: List[Tensor]\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            objectness (Tensor)\n            pred_bbox_deltas (Tensor)\n            labels (List[Tensor])\n            regression_targets (List[Tensor])\n\n        Returns:\n            objectness_loss (Tensor)\n            box_loss (Tensor)\n        \"\"\"\n\n        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n        sampled_pos_inds = torch.where(torch.cat(sampled_pos_inds, dim=0))[0]\n        sampled_neg_inds = torch.where(torch.cat(sampled_neg_inds, dim=0))[0]\n\n        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n\n        objectness = objectness.flatten()\n\n        labels = torch.cat(labels, dim=0)\n        regression_targets = torch.cat(regression_targets, dim=0)\n\n        box_loss = F.mse_loss(\n            pred_bbox_deltas[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            reduction=\"sum\",\n        ) / (sampled_inds.numel())\n\n        normalization_factor = max(1, labels.numel())  # Utilizza almeno 1 per evitare la divisione per zero\n    \n        # Controlla NaN\n        if torch.isnan(box_loss):\n            # Assegna un valore predefinito o gestisci la situazione di conseguenza\n            box_loss = torch.tensor(0.0)  # Ad esempio, assegnando 0.0 in caso di NaN\n\n        box_loss = box_loss / normalization_factor\n\n        objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n        return objectness_loss, box_loss","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.719719Z","iopub.execute_input":"2024-01-05T14:13:13.720279Z","iopub.status.idle":"2024-01-05T14:13:13.735101Z","shell.execute_reply.started":"2024-01-05T14:13:13.720247Z","shell.execute_reply":"2024-01-05T14:13:13.734349Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TwoMLPHead(nn.Module):\n    \"\"\"\n    Standard heads for FPN-based models\n \n    Args:\n        in_channels (int): number of input channels\n        representation_size (int): size of the intermediate representation\n    \"\"\"\n \n    def __init__(self, in_channels, representation_size):\n        super().__init__()\n \n        self.fc6 = nn.Linear(in_channels, representation_size)\n        self.fc7 = nn.Linear(representation_size, representation_size)\n \n    def forward(self, x):\n        x = x.flatten(start_dim=1)\n \n        x = F.relu(self.fc6(x))\n        x = F.relu(self.fc7(x))\n \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.736112Z","iopub.execute_input":"2024-01-05T14:13:13.736393Z","iopub.status.idle":"2024-01-05T14:13:13.749379Z","shell.execute_reply.started":"2024-01-05T14:13:13.736371Z","shell.execute_reply":"2024-01-05T14:13:13.748492Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class FastRCNNPredictor(nn.Module):\n    \"\"\"\n    Standard classification + bounding box regression layers\n    for Fast R-CNN.\n \n    Args:\n        in_channels (int): number of input channels\n        num_classes (int): number of output classes (including background)\n    \"\"\"\n \n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.cls_score = nn.Linear(in_channels, num_classes)\n        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n \n    def forward(self, x):\n        if x.dim() == 4:\n            torch._assert(\n                list(x.shape[2:]) == [1, 1],\n                f\"x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}\",\n            )\n        x = x.flatten(start_dim=1)\n        scores = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n \n        return scores, bbox_deltas","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.750484Z","iopub.execute_input":"2024-01-05T14:13:13.750762Z","iopub.status.idle":"2024-01-05T14:13:13.766537Z","shell.execute_reply.started":"2024-01-05T14:13:13.750738Z","shell.execute_reply":"2024-01-05T14:13:13.765775Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def iou_loss(pred_boxes, target_boxes):\n    # Calcola l'area dell'ancora\n    anchor_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n    # Calcola l'area della predizione\n    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n\n    # Calcola l'area di intersezione\n    inter_xmin = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n    inter_ymin = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n    inter_xmax = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n    inter_ymax = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n\n    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n    intersection = inter_width * inter_height\n\n    # Calcola l'area di unione\n    union = anchor_area + pred_area - intersection\n\n    # Calcola la IoU Loss evitando la divisione per zero\n    iou = torch.where(union > 0, intersection / union, torch.tensor(0.0))\n\n    # Calcola la perdita finale\n    iou_loss = 1 - iou\n\n    return iou_loss.mean()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.771111Z","iopub.execute_input":"2024-01-05T14:13:13.771729Z","iopub.status.idle":"2024-01-05T14:13:13.780832Z","shell.execute_reply.started":"2024-01-05T14:13:13.771704Z","shell.execute_reply":"2024-01-05T14:13:13.780059Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n    \"\"\"\n    Computes the loss for Faster R-CNN.\n\n    Args:\n        class_logits (Tensor)\n        box_regression (Tensor)\n        labels (list[BoxList])\n        regression_targets (Tensor)\n\n    Returns:\n        classification_loss (Tensor)\n        box_loss (Tensor)\n    \"\"\"\n\n    labels = torch.cat(labels, dim=0)\n    regression_targets = torch.cat(regression_targets, dim=0)\n\n    classification_loss = F.cross_entropy(class_logits, labels)\n\n    # get indices that correspond to the regression targets for\n    # the corresponding ground truth labels, to be used with\n    # advanced indexing\n    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n    labels_pos = labels[sampled_pos_inds_subset]\n    N, num_classes = class_logits.shape\n    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n\n    box_loss = iou_loss(\n        box_regression[sampled_pos_inds_subset, labels_pos],\n        regression_targets[sampled_pos_inds_subset]\n    )\n    normalization_factor = max(1, labels.numel())  # Utilizza almeno 1 per evitare la divisione per zero\n    box_loss = box_loss / normalization_factor\n\n    # Controlla NaN\n    if torch.isnan(box_loss):\n        box_loss = torch.tensor(0.0)\n\n    # Restituisce le perdite\n    return classification_loss, box_loss\n'''    \n    box_loss = box_loss / labels.numel()\n\n    return classification_loss, box_loss\n\n'''\n\nclass CustomRoIHeads(RoIHeads):\n    def __init__(\n        self,\n        box_roi_pool,\n        box_head,\n        box_predictor,\n        # Faster R-CNN training\n        fg_iou_thresh,\n        bg_iou_thresh,\n        batch_size_per_image,\n        positive_fraction,\n        bbox_reg_weights,\n        # Faster R-CNN inference\n        score_thresh,\n        nms_thresh,\n        detections_per_img,\n        # Mask\n        mask_roi_pool=None,\n        mask_head=None,\n        mask_predictor=None,\n        keypoint_roi_pool=None,\n        keypoint_head=None,\n        keypoint_predictor=None,\n    ):\n        super().__init__(\n            box_roi_pool,\n            box_head,\n            box_predictor,\n            # Faster R-CNN training\n            fg_iou_thresh,\n            bg_iou_thresh,\n            batch_size_per_image,\n            positive_fraction,\n            bbox_reg_weights,\n            # Faster R-CNN inference\n            score_thresh,\n            nms_thresh,\n            detections_per_img,\n        )\n        \n    def forward(\n        self,\n        features,  # type: Dict[str, Tensor]\n        proposals,  # type: List[Tensor]\n        image_shapes,  # type: List[Tuple[int, int]]\n        targets=None,  # type: Optional[List[Dict[str, Tensor]]]\n    ):\n        # type: (...) -> Tuple[List[Dict[str, Tensor]], Dict[str, Tensor]]\n        \"\"\"\n        Args:\n            features (List[Tensor])\n            proposals (List[Tensor[N, 4]])\n            image_shapes (List[Tuple[H, W]])\n            targets (List[Dict])\n        \"\"\"\n        if targets is not None:\n            for t in targets:\n                # TODO: https://github.com/pytorch/pytorch/issues/26731\n                floating_point_types = (torch.float, torch.double, torch.half)\n                if not t[\"boxes\"].dtype in floating_point_types:\n                    raise TypeError(f\"target boxes must of float type, instead got {t['boxes'].dtype}\")\n                if not t[\"labels\"].dtype == torch.int64:\n                    raise TypeError(f\"target labels must of int64 type, instead got {t['labels'].dtype}\")\n                if self.has_keypoint():\n                    if not t[\"keypoints\"].dtype == torch.float32:\n                        raise TypeError(f\"target keypoints must of float type, instead got {t['keypoints'].dtype}\")\n\n        if self.training:\n            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n        else:\n            labels = None\n            regression_targets = None\n            matched_idxs = None\n\n        box_features = self.box_roi_pool(features, proposals, image_shapes)\n        box_features = self.box_head(box_features)\n        class_logits, box_regression = self.box_predictor(box_features)\n\n        result: List[Dict[str, torch.Tensor]] = []\n        losses = {}\n        if self.training:\n            if labels is None:\n                raise ValueError(\"labels cannot be None\")\n            if regression_targets is None:\n                raise ValueError(\"regression_targets cannot be None\")\n            loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n            losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n        else:\n            boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n            num_images = len(boxes)\n            for i in range(num_images):\n                result.append(\n                    {\n                        \"boxes\": boxes[i],\n                        \"labels\": labels[i],\n                        \"scores\": scores[i],\n                    }\n                )\n\n        if self.has_mask():\n            mask_proposals = [p[\"boxes\"] for p in result]\n            if self.training:\n                if matched_idxs is None:\n                    raise ValueError(\"if in training, matched_idxs should not be None\")\n\n                # during training, only focus on positive boxes\n                num_images = len(proposals)\n                mask_proposals = []\n                pos_matched_idxs = []\n                for img_id in range(num_images):\n                    pos = torch.where(labels[img_id] > 0)[0]\n                    mask_proposals.append(proposals[img_id][pos])\n                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n            else:\n                pos_matched_idxs = None\n\n            if self.mask_roi_pool is not None:\n                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)\n                mask_features = self.mask_head(mask_features)\n                mask_logits = self.mask_predictor(mask_features)\n            else:\n                raise Exception(\"Expected mask_roi_pool to be not None\")\n\n            loss_mask = {}\n            if self.training:\n                if targets is None or pos_matched_idxs is None or mask_logits is None:\n                    raise ValueError(\"targets, pos_matched_idxs, mask_logits cannot be None when training\")\n\n                gt_masks = [t[\"masks\"] for t in targets]\n                gt_labels = [t[\"labels\"] for t in targets]\n                rcnn_loss_mask = maskrcnn_loss(mask_logits, mask_proposals, gt_masks, gt_labels, pos_matched_idxs)\n                loss_mask = {\"loss_mask\": rcnn_loss_mask}\n            else:\n                labels = [r[\"labels\"] for r in result]\n                masks_probs = maskrcnn_inference(mask_logits, labels)\n                for mask_prob, r in zip(masks_probs, result):\n                    r[\"masks\"] = mask_prob\n\n            losses.update(loss_mask)\n\n        # keep none checks in if conditional so torchscript will conditionally\n        # compile each branch\n        if (\n            self.keypoint_roi_pool is not None\n            and self.keypoint_head is not None\n            and self.keypoint_predictor is not None\n        ):\n            keypoint_proposals = [p[\"boxes\"] for p in result]\n            if self.training:\n                # during training, only focus on positive boxes\n                num_images = len(proposals)\n                keypoint_proposals = []\n                pos_matched_idxs = []\n                if matched_idxs is None:\n                    raise ValueError(\"if in trainning, matched_idxs should not be None\")\n\n                for img_id in range(num_images):\n                    pos = torch.where(labels[img_id] > 0)[0]\n                    keypoint_proposals.append(proposals[img_id][pos])\n                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n            else:\n                pos_matched_idxs = None\n\n            keypoint_features = self.keypoint_roi_pool(features, keypoint_proposals, image_shapes)\n            keypoint_features = self.keypoint_head(keypoint_features)\n            keypoint_logits = self.keypoint_predictor(keypoint_features)\n\n            loss_keypoint = {}\n            if self.training:\n                if targets is None or pos_matched_idxs is None:\n                    raise ValueError(\"both targets and pos_matched_idxs should not be None when in training mode\")\n\n                gt_keypoints = [t[\"keypoints\"] for t in targets]\n                rcnn_loss_keypoint = keypointrcnn_loss(\n                    keypoint_logits, keypoint_proposals, gt_keypoints, pos_matched_idxs\n                )\n                loss_keypoint = {\"loss_keypoint\": rcnn_loss_keypoint}\n            else:\n                if keypoint_logits is None or keypoint_proposals is None:\n                    raise ValueError(\n                        \"both keypoint_logits and keypoint_proposals should not be None when not in training mode\"\n                    )\n\n                keypoints_probs, kp_scores = keypointrcnn_inference(keypoint_logits, keypoint_proposals)\n                for keypoint_prob, kps, r in zip(keypoints_probs, kp_scores, result):\n                    r[\"keypoints\"] = keypoint_prob\n                    r[\"keypoints_scores\"] = kps\n            losses.update(loss_keypoint)\n\n        return result, losses","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.781980Z","iopub.execute_input":"2024-01-05T14:13:13.782249Z","iopub.status.idle":"2024-01-05T14:13:13.815460Z","shell.execute_reply.started":"2024-01-05T14:13:13.782227Z","shell.execute_reply":"2024-01-05T14:13:13.814465Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name='resnet50', num_classes=60):\n    \"\"\"\n    Restituisce un modello Faster R-CNN con una determinata architettura di backbone.\n\n    Precondizioni:\n        - `model_name` è una stringa che specifica l'architettura del backbone. \n          Default è 'resnet50'.\n        - `num_classes` è il numero di classi dell'insieme di dati. Default è 60.\n\n    Postcondizioni:\n        - Restituisce un modello Faster R-CNN con il backbone specificato e il numero di classi.\n    \"\"\"\n    backbone = resnet_fpn_backbone(model_name, pretrained=True)\n    model = CustomFasterRCNN(backbone, num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.816768Z","iopub.execute_input":"2024-01-05T14:13:13.817090Z","iopub.status.idle":"2024-01-05T14:13:13.829515Z","shell.execute_reply.started":"2024-01-05T14:13:13.817056Z","shell.execute_reply":"2024-01-05T14:13:13.828640Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di aggregazione per un batch di campioni.\n\n    Precondizioni:\n        - `batch` è una lista di campioni, ciascuno nel formato (immagine, target).\n          Dove 'immagine' è un tensore rappresentante l'immagine e 'target' è un dizionario\n          contenente le etichette associate all'immagine.\n\n    Postcondizioni:\n        - Restituisce una tupla di due elementi:\n          1. Un tensore contenente tutte le immagini del batch.\n          2. Una lista di dizionari rappresentanti i target corrispondenti alle immagini.\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.830717Z","iopub.execute_input":"2024-01-05T14:13:13.831041Z","iopub.status.idle":"2024-01-05T14:13:13.840355Z","shell.execute_reply.started":"2024-01-05T14:13:13.831006Z","shell.execute_reply":"2024-01-05T14:13:13.839545Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        \"\"\"\n        Inizializza un oggetto Averager.\n\n        Postcondizioni:\n            - `current_total` è l'accumulo corrente dei valori.\n            - `iterations` è il numero corrente di iterazioni.\n        \"\"\"\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        \"\"\"\n        Aggiunge un valore all'accumulo e incrementa il numero di iterazioni.\n\n        Precondizioni:\n            - `value` è il valore da aggiungere all'accumulo.\n\n        Postcondizioni:\n            - Aggiunge `value` all'accumulo.\n            - Incrementa il numero di iterazioni.\n        \"\"\"\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        \"\"\"\n        Calcola e restituisce la media dei valori finora.\n\n        Postcondizioni:\n            - Restituisce la media dei valori finora.\n              Se `iterations` è 0, restituisce 0.\n        \"\"\"\n        print(f'value - self.iterations {self.iterations}, self.current_total {self.current_total}')\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        \"\"\"\n        Reimposta l'accumulo e il numero di iterazioni a zero.\n\n        Postcondizioni:\n            - `current_total` è reimpostato a 0.\n            - `iterations` è reimpostato a 0.\n        \"\"\"\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.841515Z","iopub.execute_input":"2024-01-05T14:13:13.841874Z","iopub.status.idle":"2024-01-05T14:13:13.853777Z","shell.execute_reply.started":"2024-01-05T14:13:13.841836Z","shell.execute_reply":"2024-01-05T14:13:13.852895Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# DEFINIZIONE DEL MODELLO","metadata":{}},{"cell_type":"code","source":"# Definisci la tua implementazione personalizzata della FasterRCNN\nclass CustomFasterRCNN(FasterRCNN):\n    def __init__(\n        self,\n        backbone,\n        num_classes=None,\n        # transform parameters\n        min_size=800,\n        max_size=1333,\n        image_mean=None,\n        image_std=None,\n        # RPN parameters\n        rpn_anchor_generator=None,\n        rpn_head=None,\n        rpn_pre_nms_top_n_train=2000,\n        rpn_pre_nms_top_n_test=1000,\n        rpn_post_nms_top_n_train=2000,\n        rpn_post_nms_top_n_test=1000,\n        rpn_nms_thresh=0.7,\n        rpn_fg_iou_thresh=0.7,\n        rpn_bg_iou_thresh=0.3,\n        rpn_batch_size_per_image=256,\n        rpn_positive_fraction=0.5,\n        rpn_score_thresh=0.0,\n        # Box parameters\n        box_roi_pool=None,\n        box_head=None,\n        box_predictor=None,\n        box_score_thresh=0.05,\n        box_nms_thresh=0.5,\n        box_detections_per_img=100,\n        box_fg_iou_thresh=0.5,\n        box_bg_iou_thresh=0.5,\n        box_batch_size_per_image=512,\n        box_positive_fraction=0.25,\n        bbox_reg_weights=None,\n        **kwargs,\n    ):\n        # Implementa il tuo costruttore personalizzato\n        super().__init__(\n            backbone, num_classes            \n        )\n        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n        \n        if not hasattr(backbone, \"out_channels\"):\n            raise ValueError(\n                \"backbone should contain an attribute out_channels \"\n                \"specifying the number of output channels (assumed to be the \"\n                \"same for all the levels)\"\n            )\n \n        if not isinstance(rpn_anchor_generator, (AnchorGenerator, type(None))):\n            raise TypeError(\n                f\"rpn_anchor_generator should be of type AnchorGenerator or None instead of {type(rpn_anchor_generator)}\"\n            )\n \n        if num_classes is not None:\n            if box_predictor is not None:\n                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n        else:\n            if box_predictor is None:\n                raise ValueError(\"num_classes should not be None when box_predictor is not specified\")\n \n        out_channels = backbone.out_channels\n \n        if rpn_anchor_generator is None:\n            rpn_anchor_generator = _default_anchorgen()\n        if rpn_head is None:\n            rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n        \n        # Sostituisci la RPN predefinita con la tua implementazione personalizzata\n        self.rpn = CustomRegionProposalNetwork(\n            rpn_anchor_generator,\n            rpn_head,\n            rpn_fg_iou_thresh,\n            rpn_bg_iou_thresh,\n            rpn_batch_size_per_image,\n            rpn_positive_fraction,\n            rpn_pre_nms_top_n,\n            rpn_post_nms_top_n,\n            rpn_nms_thresh,\n            score_thresh=rpn_score_thresh,\n        )\n        \n        '''\n        if box_roi_pool is None:\n            box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)\n\n        if box_head is None:\n            resolution = box_roi_pool.output_size[0]\n            representation_size = 1024\n            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)\n\n        if box_predictor is None:\n            representation_size = 1024\n            box_predictor = FastRCNNPredictor(representation_size, num_classes)\n\n        self.roi_heads = CustomRoIHeads(\n            # Box\n            box_roi_pool,\n            box_head,\n            box_predictor,\n            box_fg_iou_thresh,\n            box_bg_iou_thresh,\n            box_batch_size_per_image,\n            box_positive_fraction,\n            bbox_reg_weights,\n            box_score_thresh,\n            box_nms_thresh,\n            box_detections_per_img,\n        )\n        '''","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.855118Z","iopub.execute_input":"2024-01-05T14:13:13.855628Z","iopub.status.idle":"2024-01-05T14:13:13.869417Z","shell.execute_reply.started":"2024-01-05T14:13:13.855581Z","shell.execute_reply":"2024-01-05T14:13:13.868630Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Crea il modello Faster R-CNN\nmodel_name = 'resnet50'\nnum_classes = 60\nmodel = get_model(model_name, num_classes)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:13.870654Z","iopub.execute_input":"2024-01-05T14:13:13.870922Z","iopub.status.idle":"2024-01-05T14:13:15.160830Z","shell.execute_reply.started":"2024-01-05T14:13:13.870900Z","shell.execute_reply":"2024-01-05T14:13:15.159932Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 236MB/s] \n","output_type":"stream"},{"name":"stdout","text":"CustomFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): CustomRegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=60, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=240, bias=True)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configurazione della trasformazione per l'oggetto model\nmodel.transform = GeneralizedRCNNTransform(\n    min_size=(640,),             # Dimensione minima dell'immagine durante la trasformazione\n    max_size=640,                 # Dimensione massima dell'immagine durante la trasformazione\n    image_mean=[0.485, 0.456, 0.406],   # Media dell'immagine per la normalizzazione\n    image_std=[0.229, 0.224, 0.225]     # Deviazione standard dell'immagine per la normalizzazione\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:15.161954Z","iopub.execute_input":"2024-01-05T14:13:15.162253Z","iopub.status.idle":"2024-01-05T14:13:15.167315Z","shell.execute_reply.started":"2024-01-05T14:13:15.162201Z","shell.execute_reply":"2024-01-05T14:13:15.166412Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Sposta il modello sulla GPU, se disponibile\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:15.168415Z","iopub.execute_input":"2024-01-05T14:13:15.168722Z","iopub.status.idle":"2024-01-05T14:13:15.416943Z","shell.execute_reply.started":"2024-01-05T14:13:15.168699Z","shell.execute_reply":"2024-01-05T14:13:15.415958Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_losses = Averager()     # Accumulatore di loss per train\nval_losses = Averager()       # Accumulatore di loss per la validazione\n\nbest_loss = float('inf')       # Migliore loss inizializzata a infinito\nos.makedirs(weight_path, exist_ok=True)  # Creazione della directory per i pesi del modello, se non esiste già","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:15.418109Z","iopub.execute_input":"2024-01-05T14:13:15.418400Z","iopub.status.idle":"2024-01-05T14:13:15.423510Z","shell.execute_reply.started":"2024-01-05T14:13:15.418376Z","shell.execute_reply":"2024-01-05T14:13:15.422516Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# IPERPARAMETRI\nnum_epochs = 10\nbatch_size = 16\nnum_workers = 4\nmomentum = 0.9\nlearning_rate = 0.005\n\n# Pesi per la somma pesata delle loss \nrpn_loss_regr_weight = 1     # box regression della region proposal network\nrpn_loss_cls_fixed_num = 1   # object classification della rpn\nclass_loss_cls_weight = 1    # object classfication della parte finale della rete\nclass_loss_regr_weight = 1   # box regression per la parte finale della rete","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:15:00.552628Z","iopub.execute_input":"2024-01-05T14:15:00.553099Z","iopub.status.idle":"2024-01-05T14:15:00.558569Z","shell.execute_reply.started":"2024-01-05T14:15:00.553065Z","shell.execute_reply":"2024-01-05T14:15:00.557803Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Definisci i percorsi ai tuoi dataset\ntrain_dataset = BuildingsDataset(train_path, coco_path)\nval_dataset = BuildingsDataset(val_path, coco_path)\ntest_dataset = BuildingsDataset(test_path, coco_path)\n \n# Crea i data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n\n# Definizione dell'ottimizzatore\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) \n#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-5)\n#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:15:02.475940Z","iopub.execute_input":"2024-01-05T14:15:02.476539Z","iopub.status.idle":"2024-01-05T14:15:08.007219Z","shell.execute_reply.started":"2024-01-05T14:15:02.476508Z","shell.execute_reply":"2024-01-05T14:15:08.006098Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"lossModel_Train = []\nlossModel_Val = []\n        \n# Addestramento\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses.reset()\n    val_losses.reset()\n    \n    # Iterazione su batches di addestramento\n    for batch_index, (images, targets) in enumerate(tqdm(train_loader)):\n        images, targets = [image.to(device) for image in images], [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        model.zero_grad()\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        \n        for key, value in loss_dict.items():\n            if torch.isnan(value):\n                loss_dict[key] = torch.tensor(0.0)\n        \n        # Calcolo della loss totale come combinazione pesata di diverse componenti\n        loss = rpn_loss_regr_weight * loss_dict['loss_rpn_box_reg'] +\\\n               rpn_loss_cls_fixed_num * loss_dict['loss_objectness'] +\\\n               class_loss_cls_weight * loss_dict['loss_classifier'] +\\\n               class_loss_regr_weight * loss_dict['loss_box_reg']\n            \n        # Tracciamento della loss\n        train_losses.send(loss.item())\n\n        # Backpropagation e aggiornamento dei pesi\n        loss.backward()\n        optimizer.step()\n        \n        # Stampa della loss ogni 50 batches\n        '''if batch_index % 50 == 0:\n            print(f\"TRAINING: Epoch: {epoch} Batch Index: {batch_index} Loss: {train_losses.value}\")\n            modelLoss_train = train_losses.value'''\n            \n    modelLoss_train = train_losses.value        \n    print(f\"TRAINING: Epoch: {epoch+1} Loss: {modelLoss_train}\")   \n    \n    # Validazione\n    with torch.no_grad():\n        for batch_index, (images, targets) in enumerate(tqdm(val_loader)):\n            images, targets = [image.to(device) for image in images], [{k: v.to(device) for k, v in t.items()} for t in targets]\n            outputs = model(images, targets)\n  \n            for key, value in outputs.items():\n                if torch.isnan(value):\n                    outputs[key] = torch.tensor(0.0)\n\n            # Calcolo della loss totale durante la validazione\n            loss = rpn_loss_regr_weight * outputs['loss_rpn_box_reg'] +\\\n                    rpn_loss_cls_fixed_num * outputs['loss_objectness'] +\\\n                    class_loss_cls_weight * outputs['loss_classifier'] +\\\n                    class_loss_regr_weight * outputs['loss_box_reg']\n\n            # Tracciamento della loss\n            val_losses.send(loss.item())\n            '''\n            # Stampa della loss di validazione ogni 50 batches\n            if batch_index % 50 == 0:\n                print(f\"VALIDATION: Epoch: {epoch} Batch Index: {batch_index} Loss: {val_losses.value}\")\n                modelLoss_val = val_losses.value \n            '''    \n        modelLoss_val = val_losses.value        \n        print(f\"VALIDATION: Epoch: {epoch+1} Loss: {modelLoss_val}\")\n        \n        # Salvataggio dei migliori pesi se la loss di validazione è migliorata    \n        if (modelLoss_val < best_loss) :\n            print('     .... Saving best weights ....')\n            best_loss = modelLoss_val\n            #salvataggio dei migliori pesi sul validation\n            torch.save(model.state_dict(), weight_path + 'best_model_weights.pth')\n        \n        # Salvataggio delle loss per i plot\n        lossModel_Train.append(modelLoss_train)\n        lossModel_Val.append(modelLoss_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:15:30.165300Z","iopub.execute_input":"2024-01-05T14:15:30.166225Z","iopub.status.idle":"2024-01-05T14:15:33.942498Z","shell.execute_reply.started":"2024-01-05T14:15:30.166190Z","shell.execute_reply":"2024-01-05T14:15:33.941116Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"  0%|          | 0/1109 [00:03<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m train_losses\u001b[38;5;241m.\u001b[39msend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Backpropagation e aggiornamento dei pesi\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Stampa della loss ogni 50 batches\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}]},{"cell_type":"code","source":"# Effettuiamo il plot dele curve di loss\nplt.figure()\nplt.title(\"Model: Training Vs Validation Losses\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(list(range(1,len(lossModel_Train)+1)), lossModel_Train, color='r', label=\"Training Loss\")\nplt.plot(list(range(1, len(lossModel_Val)+1)), lossModel_Val, color='g', label=\"Validation Loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.402039Z","iopub.status.idle":"2024-01-05T14:13:50.402386Z","shell.execute_reply.started":"2024-01-05T14:13:50.402205Z","shell.execute_reply":"2024-01-05T14:13:50.402222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test di Prova","metadata":{}},{"cell_type":"code","source":"def visualize_results_with_boxes(image_path, results):\n    bounding_boxes = results['boxes']\n\n    # Carica l'immagine usando PIL\n    image = Image.open(image_path)\n\n    # Inizializza ImageDraw con l'immagine\n    draw = ImageDraw.Draw(image)\n\n    # Disegna i bounding box sull'immagine\n    for bbox in bounding_boxes:\n        bbox_coords = bbox  # Converti la stringa bbox in una lista di coordinate\n        bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[2],bbox_coords[3]]\n        draw.rectangle(bbox_coords, outline='red', width=2)\n\n    # Visualizza l'immagine con i bounding box\n    plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.403677Z","iopub.status.idle":"2024-01-05T14:13:50.404015Z","shell.execute_reply.started":"2024-01-05T14:13:50.403853Z","shell.execute_reply":"2024-01-05T14:13:50.403869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = sorted(os.listdir(test_path))\nprint(test_images[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.405775Z","iopub.status.idle":"2024-01-05T14:13:50.406118Z","shell.execute_reply.started":"2024-01-05T14:13:50.405949Z","shell.execute_reply":"2024-01-05T14:13:50.405965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_image_with_boxes(image_path, coco_annotations_path):\n    # Carica il file COCO annotations JSON\n    with open(coco_annotations_path, 'r') as coco_file:\n        coco_data = json.load(coco_file)\n\n    # Estrai l'id dell'immagine dal nome del file\n    image_name = image_path.split(\"/\")[-1]\n    image_id = next((img['id'] for img in coco_data['images'] if img['file_name'] == image_name), None)\n\n    # Se l'id dell'immagine è trovato, estrai i bounding box corrispondenti\n    if image_id is not None:\n        bounding_boxes = [bbox for bbox in coco_data['annotations'] if bbox['image_id'] == image_id]\n        \n        # Carica l'immagine usando PIL\n        image = Image.open(image_path)\n\n        # Inizializza ImageDraw con l'immagine\n        draw = ImageDraw.Draw(image)\n\n        # Disegna i bounding box sull'immagine\n        for bbox in bounding_boxes:\n            bbox_coords = eval(bbox['bbox'])  # Converti la stringa bbox in una lista di coordinate\n            bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[0]+bbox_coords[2],bbox_coords[1]+bbox_coords[3]]\n            draw.rectangle(bbox_coords, outline='red', width=2)\n\n        # Visualizza l'immagine con i bounding box\n        plt.imshow(image)\n        plt.show()\n        return image\n    else:\n        print(f\"Image {image_name} not found in COCO annotations.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.407549Z","iopub.status.idle":"2024-01-05T14:13:50.407898Z","shell.execute_reply.started":"2024-01-05T14:13:50.407740Z","shell.execute_reply":"2024-01-05T14:13:50.407756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carichiamo i pesi relativi al modello migliore\n#weights = torch.load('/kaggle/input/dataset-ipcv-teama/best_model_weights_rpn_10epoch_16batch_0.005lr.pth')\nweights = torch.load(weight_path + 'best_model_weights.pth')\nmodel.load_state_dict(weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.409116Z","iopub.status.idle":"2024-01-05T14:13:50.409434Z","shell.execute_reply.started":"2024-01-05T14:13:50.409274Z","shell.execute_reply":"2024-01-05T14:13:50.409288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indice = 50\n# Test su un'immagine\nmodel.eval()\ntest_image = test_dataset[indice][0]\n#print(test_image)\n\ntest_image = test_image.to(device)\ntest_image = test_image.unsqueeze(0)\nwith torch.no_grad():\n    output = model(test_image)\n    \nsoglia = 0.1\noutput_tagliato = []\n \nfor detection in output:\n    boxes = detection['boxes']\n    labels = detection['labels']\n    scores = detection['scores']\n \n    # Trova gli indici dei box che superano la soglia\n    indici_superati_soglia = scores >= soglia\n \n    # Filtra i box, le label e gli score in base agli indici superati la soglia\n    boxes_tagliati = boxes[indici_superati_soglia]\n    labels_tagliati = labels[indici_superati_soglia]\n    scores_tagliati = scores[indici_superati_soglia]\n \n    # Creare un nuovo dizionario con i risultati tagliati\n    detection_tagliata = {\n        'boxes': boxes_tagliati,\n        'labels': labels_tagliati,\n        'scores': scores_tagliati\n    }\n \n    # Aggiungere il risultato alla lista finale solo se ci sono box sopra la soglia\n    if len(boxes_tagliati) > 0:\n        output_tagliato.append(detection_tagliata)\n\noriginal_image_path = test_path + '/' + test_images[indice]\nimg = visualize_image_with_boxes(original_image_path, coco_path)\nvisualize_results_with_boxes(original_image_path, output_tagliato[0])   ","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:13:50.410905Z","iopub.status.idle":"2024-01-05T14:13:50.411212Z","shell.execute_reply.started":"2024-01-05T14:13:50.411061Z","shell.execute_reply":"2024-01-05T14:13:50.411075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}