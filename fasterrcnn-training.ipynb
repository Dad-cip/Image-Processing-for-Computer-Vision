{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7345777,"sourceType":"datasetVersion","datasetId":4212640}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:17.933541Z","iopub.execute_input":"2024-01-07T11:55:17.933933Z","iopub.status.idle":"2024-01-07T11:55:30.814078Z","shell.execute_reply.started":"2024-01-07T11:55:17.933902Z","shell.execute_reply":"2024-01-07T11:55:30.812878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np                                    # Importa Numpy\nimport skimage.io as io                               # Importa il modulo Input/ouput di SK-Image\nfrom skimage.transform import resize                  # Importa il modulo resize da SK-Image\nfrom os import listdir                                # Importa il modulo listdir da OS\n\nimport json                                           # Importa Json\nfrom matplotlib.collections import PatchCollection    # Importa PatchCollection dal modulo collections di MatPlotLib\nfrom pycocotools.coco import COCO                     # Importa COCO dal modulo coco di PyCoco-Tools\nimport pycocotools.mask as cocomask                   # Importa il modulo Mask di PyCoco-Tools\nimport matplotlib.pyplot as plt                       # Importa il modulo  pyplot di MatPlotLib\n\nimport PIL\nfrom PIL import Image, ImageDraw                      # Importa il modulo Image da PIL\n\nfrom tensorflow import keras                          # Importa il modulo Keras di TensorFlow\nimport os                                             # Importa os\n\nfrom tqdm import tqdm                                 # Importa il modulo tqdm da tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.transforms import transforms\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\nfrom torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\nfrom torchvision.models.detection.roi_heads import RoIHeads\nfrom torchvision.ops import MultiScaleRoIAlign\n\nfrom typing import Dict, List, Optional, Tuple\n\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:30.816773Z","iopub.execute_input":"2024-01-07T11:55:30.817214Z","iopub.status.idle":"2024-01-07T11:55:45.145980Z","shell.execute_reply.started":"2024-01-07T11:55:30.817175Z","shell.execute_reply":"2024-01-07T11:55:45.145158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path\npath_in = '/kaggle/input/dataset-ipcv-teama/'\npath_out = '/kaggle/working/'\n\ncoco_path = path_in + 'COCO_annotations.json/COCO_annotations.json'\ntrain_path = path_in + 'dataset_IPCV/kaggle/working/train'\ntest_path = path_in + 'dataset_IPCV/kaggle/working/test'\nval_path = path_in + 'dataset_IPCV/kaggle/working/val'\n\nweight_path = path_out + 'weight/'","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.147202Z","iopub.execute_input":"2024-01-07T11:55:45.147941Z","iopub.status.idle":"2024-01-07T11:55:45.156806Z","shell.execute_reply.started":"2024-01-07T11:55:45.147902Z","shell.execute_reply":"2024-01-07T11:55:45.155894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Def. di Funzioni","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n    \"\"\"\n    Carica un'immagine da un percorso specificato.\n\n    Precondizione:\n        - `image_path` è il percorso completo dell'immagine da caricare.\n\n    Postcondizione:\n        - Restituisce l'immagine caricata.\n        - Solleva un'eccezione AssertionError se l'immagine non viene trovata nel percorso specificato.\n    \"\"\"\n    image = plt.imread(image_path)\n    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.158406Z","iopub.execute_input":"2024-01-07T11:55:45.158846Z","iopub.status.idle":"2024-01-07T11:55:45.192137Z","shell.execute_reply.started":"2024-01-07T11:55:45.158790Z","shell.execute_reply":"2024-01-07T11:55:45.191110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BuildingsDataset(Dataset):\n    def __init__(self, images_dir, annotation_path, transform=None):\n        \"\"\"\n        Inizializza un oggetto BuildingsDataset.\n\n        Precondizioni:\n            - `images_dir` è il percorso della directory contenente le immagini.\n            - `annotation_path` è il percorso del file JSON contenente le annotazioni.\n            - `transform` è un oggetto trasformazione (es. da torchvision.transforms) per applicare\n              trasformazioni alle immagini e alle etichette, opzionale.\n\n        Postcondizioni:\n            - `image_paths` è una lista di percorsi completi delle immagini nella directory `images_dir`.\n            - `annotations` contiene le annotazioni caricate dal file JSON specificato.\n            - `transform` è l'oggetto trasformazione fornito.\n        \"\"\"\n        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n        self.transform = transform\n \n        with open(annotation_path, 'r') as f:\n            self.annotations = json.load(f)\n \n    def transform_image_bbox(self, image, label):\n        \"\"\"\n        Applica trasformazioni all'immagine e alle etichette, se definite.\n\n        Precondizioni:\n            - `image` è l'immagine da trasformare.\n            - `label` sono le etichette associate all'immagine.\n\n        Postcondizioni:\n            - Se `transform` è definito, applica le trasformazioni all'immagine e alle etichette.\n            - Restituisce l'immagine e le etichette trasformate.\n        \"\"\"\n        if self.transform:\n            transformed = self.transform(image=image, labels=label)\n            image = transformed['image']\n            label = transformed['labels']\n \n        image = transforms.ToTensor()(image)\n        return image, label\n \n    def __getitem__(self, index):\n        \"\"\"\n        Restituisce un campione specifico dell'insieme di dati.\n\n        Precondizioni:\n            - `index` è l'indice del campione da restituire.\n\n        Postcondizioni:\n            - Restituisce un'immagine e le relative etichette nel formato richiesto.\n              Se non ci sono annotazioni per l'immagine, restituisce un target vuoto.\n        \"\"\"\n        image_path = self.image_paths[index]\n        image_name = image_path.split(\"/\")[-1]\n        image_id = next((img['id'] for img in self.annotations['images'] if img['file_name'] == image_name), None)\n        image = load_image(image_path).astype(np.float32)\n        image /= 255.0\n        image = torch.from_numpy(image).permute(2,0,1) # change the shape from [h,w,c] to [c,h,w]  \n\n        box_lab = [anno for anno in self.annotations['annotations'] if anno['image_id'] == image_id]\n\n        if(len(box_lab) == 0):\n            target = {}\n            boxes = torch.zeros((0, 4), dtype=torch.float32) \n            target = {\n                    \"boxes\": boxes, \n                    \"labels\": torch.zeros(0, dtype=torch.int64), \n                    \"image_id\": torch.as_tensor([4])\n                     }\n        else:\n            boxes = [json.loads(record['bbox']) for record in box_lab]\n            categories = [record['category_id'] for record in box_lab]\n            boxes = np.array(boxes)\n            # change the co-ordinates into expected [x, y, x+w, y+h] format\n            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n            labels = torch.as_tensor(categories, dtype=torch.int64)\n\n            target = {\n                   \"boxes\": boxes, \n                   \"labels\": labels,\n                   \"image_id\": torch.tensor([index])\n                    }\n                \n        return image, target\n \n    def __len__(self):\n        \"\"\"\n        Restituisce il numero totale di campioni nell'insieme di dati.\n\n        Postcondizioni:\n            - Restituisce la lunghezza dell'insieme di dati.\n        \"\"\"\n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.195313Z","iopub.execute_input":"2024-01-07T11:55:45.195854Z","iopub.status.idle":"2024-01-07T11:55:45.212649Z","shell.execute_reply.started":"2024-01-07T11:55:45.195804Z","shell.execute_reply":"2024-01-07T11:55:45.211622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageList:\n    \"\"\"\n    Structure that holds a list of images (of possibly\n    varying sizes) as a single tensor.\n    This works by padding the images to the same size,\n    and storing in a field the original sizes of each image\n\n    Args:\n        tensors (tensor): Tensor containing images.\n        image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.\n    \"\"\"\n\n    def __init__(self, tensors: Tensor, image_sizes: List[Tuple[int, int]]) -> None:\n        self.tensors = tensors\n        self.image_sizes = image_sizes\n\n    def to(self, device: torch.device) -> \"ImageList\":\n        cast_tensor = self.tensors.to(device)\n        return ImageList(cast_tensor, self.image_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.213931Z","iopub.execute_input":"2024-01-07T11:55:45.214283Z","iopub.status.idle":"2024-01-07T11:55:45.225478Z","shell.execute_reply.started":"2024-01-07T11:55:45.214250Z","shell.execute_reply":"2024-01-07T11:55:45.224542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _default_anchorgen():\n    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n    return AnchorGenerator(anchor_sizes, aspect_ratios)\n \n# Definisci la tua implementazione personalizzata della RPN\nclass CustomRegionProposalNetwork(RegionProposalNetwork):\n    def __init__(\n        self,\n        anchor_generator: AnchorGenerator,\n        head: nn.Module,\n        # Faster-RCNN Training\n        fg_iou_thresh: float,\n        bg_iou_thresh: float,\n        batch_size_per_image: int,\n        positive_fraction: float,\n        # Faster-RCNN Inference\n        pre_nms_top_n: Dict[str, int],\n        post_nms_top_n: Dict[str, int],\n        nms_thresh: float,\n        score_thresh: float = 0.0,\n    ):\n        # Implementa il tuo costruttore personalizzato\n        super().__init__(\n            anchor_generator,\n            head,\n            fg_iou_thresh,\n            bg_iou_thresh,\n            batch_size_per_image,\n            positive_fraction,\n            pre_nms_top_n,\n            post_nms_top_n,\n            nms_thresh,\n            score_thresh,\n        )\n \n    def compute_loss(\n    self, objectness: Tensor, pred_bbox_deltas: Tensor, labels: List[Tensor], regression_targets: List[Tensor]\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            objectness (Tensor)\n            pred_bbox_deltas (Tensor)\n            labels (List[Tensor])\n            regression_targets (List[Tensor])\n\n        Returns:\n            objectness_loss (Tensor)\n            box_loss (Tensor)\n        \"\"\"\n\n        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n        sampled_pos_inds = torch.where(torch.cat(sampled_pos_inds, dim=0))[0]\n        sampled_neg_inds = torch.where(torch.cat(sampled_neg_inds, dim=0))[0]\n\n        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n\n        objectness = objectness.flatten()\n\n        labels = torch.cat(labels, dim=0)\n        regression_targets = torch.cat(regression_targets, dim=0)\n\n        box_loss = F.mse_loss(\n            pred_bbox_deltas[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            reduction=\"sum\",\n        ) / (sampled_inds.numel())\n\n        normalization_factor = max(1, labels.numel())  # Utilizza almeno 1 per evitare la divisione per zero\n    \n        # Controlla NaN\n        if torch.isnan(box_loss):\n            # Assegna un valore predefinito o gestisci la situazione di conseguenza\n            box_loss = torch.tensor(0.0)  # Ad esempio, assegnando 0.0 in caso di NaN\n\n        box_loss = box_loss / normalization_factor\n\n        objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n        return objectness_loss, box_loss","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.227179Z","iopub.execute_input":"2024-01-07T11:55:45.227553Z","iopub.status.idle":"2024-01-07T11:55:45.242301Z","shell.execute_reply.started":"2024-01-07T11:55:45.227520Z","shell.execute_reply":"2024-01-07T11:55:45.241395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TwoMLPHead(nn.Module):\n    \"\"\"\n    Standard heads for FPN-based models\n \n    Args:\n        in_channels (int): number of input channels\n        representation_size (int): size of the intermediate representation\n    \"\"\"\n \n    def __init__(self, in_channels, representation_size):\n        super().__init__()\n \n        self.fc6 = nn.Linear(in_channels, representation_size)\n        self.fc7 = nn.Linear(representation_size, representation_size)\n \n    def forward(self, x):\n        x = x.flatten(start_dim=1)\n \n        x = F.relu(self.fc6(x))\n        x = F.relu(self.fc7(x))\n \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.243437Z","iopub.execute_input":"2024-01-07T11:55:45.243764Z","iopub.status.idle":"2024-01-07T11:55:45.254773Z","shell.execute_reply.started":"2024-01-07T11:55:45.243738Z","shell.execute_reply":"2024-01-07T11:55:45.253871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FastRCNNPredictor(nn.Module):\n    \"\"\"\n    Standard classification + bounding box regression layers\n    for Fast R-CNN.\n \n    Args:\n        in_channels (int): number of input channels\n        num_classes (int): number of output classes (including background)\n    \"\"\"\n \n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.cls_score = nn.Linear(in_channels, num_classes)\n        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n \n    def forward(self, x):\n        if x.dim() == 4:\n            torch._assert(\n                list(x.shape[2:]) == [1, 1],\n                f\"x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}\",\n            )\n        x = x.flatten(start_dim=1)\n        scores = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n \n        return scores, bbox_deltas","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.256060Z","iopub.execute_input":"2024-01-07T11:55:45.256378Z","iopub.status.idle":"2024-01-07T11:55:45.267873Z","shell.execute_reply.started":"2024-01-07T11:55:45.256354Z","shell.execute_reply":"2024-01-07T11:55:45.266975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou_loss(pred_boxes, target_boxes):\n    # Calcola l'area dell'ancora\n    anchor_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n    # Calcola l'area della predizione\n    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n\n    # Calcola l'area di intersezione\n    inter_xmin = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n    inter_ymin = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n    inter_xmax = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n    inter_ymax = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n\n    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n    intersection = inter_width * inter_height\n\n    # Calcola l'area di unione\n    union = anchor_area + pred_area - intersection\n\n    # Calcola la IoU Loss evitando la divisione per zero\n    iou = torch.where(union > 0, intersection / union, torch.tensor(0.0))\n\n    # Calcola la perdita finale\n    iou_loss = 1 - iou\n\n    return iou_loss.mean()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.268994Z","iopub.execute_input":"2024-01-07T11:55:45.269279Z","iopub.status.idle":"2024-01-07T11:55:45.278082Z","shell.execute_reply.started":"2024-01-07T11:55:45.269246Z","shell.execute_reply":"2024-01-07T11:55:45.277165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n    \"\"\"\n    Computes the loss for Faster R-CNN.\n\n    Args:\n        class_logits (Tensor)\n        box_regression (Tensor)\n        labels (list[BoxList])\n        regression_targets (Tensor)\n\n    Returns:\n        classification_loss (Tensor)\n        box_loss (Tensor)\n    \"\"\"\n\n    labels = torch.cat(labels, dim=0)\n    regression_targets = torch.cat(regression_targets, dim=0)\n\n    classification_loss = F.cross_entropy(class_logits, labels)\n\n    # get indices that correspond to the regression targets for\n    # the corresponding ground truth labels, to be used with\n    # advanced indexing\n    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n    labels_pos = labels[sampled_pos_inds_subset]\n    N, num_classes = class_logits.shape\n    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n\n    box_loss = iou_loss(\n        box_regression[sampled_pos_inds_subset, labels_pos],\n        regression_targets[sampled_pos_inds_subset]\n    )\n    \n    '''box_loss = F.mse_loss(\n        box_regression[sampled_pos_inds_subset, labels_pos],\n        regression_targets[sampled_pos_inds_subset]\n    )'''\n    \n    normalization_factor = max(1, labels.numel())  # Utilizza almeno 1 per evitare la divisione per zero\n    box_loss = box_loss / normalization_factor\n\n    # Controlla NaN\n    if torch.isnan(box_loss):\n        box_loss = torch.tensor(0.0)\n\n    # Restituisce le perdite\n    return classification_loss, box_loss\n'''    \n    box_loss = box_loss / labels.numel()\n\n    return classification_loss, box_loss\n\n'''\n\nclass CustomRoIHeads(RoIHeads):\n    def __init__(\n        self,\n        box_roi_pool,\n        box_head,\n        box_predictor,\n        # Faster R-CNN training\n        fg_iou_thresh,\n        bg_iou_thresh,\n        batch_size_per_image,\n        positive_fraction,\n        bbox_reg_weights,\n        # Faster R-CNN inference\n        score_thresh,\n        nms_thresh,\n        detections_per_img,\n        # Mask\n        mask_roi_pool=None,\n        mask_head=None,\n        mask_predictor=None,\n        keypoint_roi_pool=None,\n        keypoint_head=None,\n        keypoint_predictor=None,\n    ):\n        super().__init__(\n            box_roi_pool,\n            box_head,\n            box_predictor,\n            # Faster R-CNN training\n            fg_iou_thresh,\n            bg_iou_thresh,\n            batch_size_per_image,\n            positive_fraction,\n            bbox_reg_weights,\n            # Faster R-CNN inference\n            score_thresh,\n            nms_thresh,\n            detections_per_img,\n        )\n        \n    def forward(\n        self,\n        features,  # type: Dict[str, Tensor]\n        proposals,  # type: List[Tensor]\n        image_shapes,  # type: List[Tuple[int, int]]\n        targets=None,  # type: Optional[List[Dict[str, Tensor]]]\n    ):\n        # type: (...) -> Tuple[List[Dict[str, Tensor]], Dict[str, Tensor]]\n        \"\"\"\n        Args:\n            features (List[Tensor])\n            proposals (List[Tensor[N, 4]])\n            image_shapes (List[Tuple[H, W]])\n            targets (List[Dict])\n        \"\"\"\n        if targets is not None:\n            for t in targets:\n                # TODO: https://github.com/pytorch/pytorch/issues/26731\n                floating_point_types = (torch.float, torch.double, torch.half)\n                if not t[\"boxes\"].dtype in floating_point_types:\n                    raise TypeError(f\"target boxes must of float type, instead got {t['boxes'].dtype}\")\n                if not t[\"labels\"].dtype == torch.int64:\n                    raise TypeError(f\"target labels must of int64 type, instead got {t['labels'].dtype}\")\n                if self.has_keypoint():\n                    if not t[\"keypoints\"].dtype == torch.float32:\n                        raise TypeError(f\"target keypoints must of float type, instead got {t['keypoints'].dtype}\")\n\n        if self.training:\n            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n        else:\n            labels = None\n            regression_targets = None\n            matched_idxs = None\n\n        box_features = self.box_roi_pool(features, proposals, image_shapes)\n        box_features = self.box_head(box_features)\n        class_logits, box_regression = self.box_predictor(box_features)\n\n        result: List[Dict[str, torch.Tensor]] = []\n        losses = {}\n        if self.training:\n            if labels is None:\n                raise ValueError(\"labels cannot be None\")\n            if regression_targets is None:\n                raise ValueError(\"regression_targets cannot be None\")\n            loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n            losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n        else:\n            boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n            num_images = len(boxes)\n            for i in range(num_images):\n                result.append(\n                    {\n                        \"boxes\": boxes[i],\n                        \"labels\": labels[i],\n                        \"scores\": scores[i],\n                    }\n                )\n\n        if self.has_mask():\n            mask_proposals = [p[\"boxes\"] for p in result]\n            if self.training:\n                if matched_idxs is None:\n                    raise ValueError(\"if in training, matched_idxs should not be None\")\n\n                # during training, only focus on positive boxes\n                num_images = len(proposals)\n                mask_proposals = []\n                pos_matched_idxs = []\n                for img_id in range(num_images):\n                    pos = torch.where(labels[img_id] > 0)[0]\n                    mask_proposals.append(proposals[img_id][pos])\n                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n            else:\n                pos_matched_idxs = None\n\n            if self.mask_roi_pool is not None:\n                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)\n                mask_features = self.mask_head(mask_features)\n                mask_logits = self.mask_predictor(mask_features)\n            else:\n                raise Exception(\"Expected mask_roi_pool to be not None\")\n\n            loss_mask = {}\n            if self.training:\n                if targets is None or pos_matched_idxs is None or mask_logits is None:\n                    raise ValueError(\"targets, pos_matched_idxs, mask_logits cannot be None when training\")\n\n                gt_masks = [t[\"masks\"] for t in targets]\n                gt_labels = [t[\"labels\"] for t in targets]\n                rcnn_loss_mask = maskrcnn_loss(mask_logits, mask_proposals, gt_masks, gt_labels, pos_matched_idxs)\n                loss_mask = {\"loss_mask\": rcnn_loss_mask}\n            else:\n                labels = [r[\"labels\"] for r in result]\n                masks_probs = maskrcnn_inference(mask_logits, labels)\n                for mask_prob, r in zip(masks_probs, result):\n                    r[\"masks\"] = mask_prob\n\n            losses.update(loss_mask)\n\n        # keep none checks in if conditional so torchscript will conditionally\n        # compile each branch\n        if (\n            self.keypoint_roi_pool is not None\n            and self.keypoint_head is not None\n            and self.keypoint_predictor is not None\n        ):\n            keypoint_proposals = [p[\"boxes\"] for p in result]\n            if self.training:\n                # during training, only focus on positive boxes\n                num_images = len(proposals)\n                keypoint_proposals = []\n                pos_matched_idxs = []\n                if matched_idxs is None:\n                    raise ValueError(\"if in trainning, matched_idxs should not be None\")\n\n                for img_id in range(num_images):\n                    pos = torch.where(labels[img_id] > 0)[0]\n                    keypoint_proposals.append(proposals[img_id][pos])\n                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n            else:\n                pos_matched_idxs = None\n\n            keypoint_features = self.keypoint_roi_pool(features, keypoint_proposals, image_shapes)\n            keypoint_features = self.keypoint_head(keypoint_features)\n            keypoint_logits = self.keypoint_predictor(keypoint_features)\n\n            loss_keypoint = {}\n            if self.training:\n                if targets is None or pos_matched_idxs is None:\n                    raise ValueError(\"both targets and pos_matched_idxs should not be None when in training mode\")\n\n                gt_keypoints = [t[\"keypoints\"] for t in targets]\n                rcnn_loss_keypoint = keypointrcnn_loss(\n                    keypoint_logits, keypoint_proposals, gt_keypoints, pos_matched_idxs\n                )\n                loss_keypoint = {\"loss_keypoint\": rcnn_loss_keypoint}\n            else:\n                if keypoint_logits is None or keypoint_proposals is None:\n                    raise ValueError(\n                        \"both keypoint_logits and keypoint_proposals should not be None when not in training mode\"\n                    )\n\n                keypoints_probs, kp_scores = keypointrcnn_inference(keypoint_logits, keypoint_proposals)\n                for keypoint_prob, kps, r in zip(keypoints_probs, kp_scores, result):\n                    r[\"keypoints\"] = keypoint_prob\n                    r[\"keypoints_scores\"] = kps\n            losses.update(loss_keypoint)\n\n        return result, losses","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.279314Z","iopub.execute_input":"2024-01-07T11:55:45.279597Z","iopub.status.idle":"2024-01-07T11:55:45.314567Z","shell.execute_reply.started":"2024-01-07T11:55:45.279574Z","shell.execute_reply":"2024-01-07T11:55:45.313599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name='resnet50', num_classes=60):\n    \"\"\"\n    Restituisce un modello Faster R-CNN con una determinata architettura di backbone.\n\n    Precondizioni:\n        - `model_name` è una stringa che specifica l'architettura del backbone. \n          Default è 'resnet50'.\n        - `num_classes` è il numero di classi dell'insieme di dati. Default è 60.\n\n    Postcondizioni:\n        - Restituisce un modello Faster R-CNN con il backbone specificato e il numero di classi.\n    \"\"\"\n    backbone = resnet_fpn_backbone(model_name, pretrained=True)\n    #model = CustomFasterRCNN(backbone, num_classes)\n    model = FasterRCNN(backbone, num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.315636Z","iopub.execute_input":"2024-01-07T11:55:45.315935Z","iopub.status.idle":"2024-01-07T11:55:45.327927Z","shell.execute_reply.started":"2024-01-07T11:55:45.315911Z","shell.execute_reply":"2024-01-07T11:55:45.327124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di aggregazione per un batch di campioni.\n\n    Precondizioni:\n        - `batch` è una lista di campioni, ciascuno nel formato (immagine, target).\n          Dove 'immagine' è un tensore rappresentante l'immagine e 'target' è un dizionario\n          contenente le etichette associate all'immagine.\n\n    Postcondizioni:\n        - Restituisce una tupla di due elementi:\n          1. Un tensore contenente tutte le immagini del batch.\n          2. Una lista di dizionari rappresentanti i target corrispondenti alle immagini.\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.329069Z","iopub.execute_input":"2024-01-07T11:55:45.329373Z","iopub.status.idle":"2024-01-07T11:55:45.339314Z","shell.execute_reply.started":"2024-01-07T11:55:45.329340Z","shell.execute_reply":"2024-01-07T11:55:45.338372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        \"\"\"\n        Inizializza un oggetto Averager.\n\n        Postcondizioni:\n            - `current_total` è l'accumulo corrente dei valori.\n            - `iterations` è il numero corrente di iterazioni.\n        \"\"\"\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        \"\"\"\n        Aggiunge un valore all'accumulo e incrementa il numero di iterazioni.\n\n        Precondizioni:\n            - `value` è il valore da aggiungere all'accumulo.\n\n        Postcondizioni:\n            - Aggiunge `value` all'accumulo.\n            - Incrementa il numero di iterazioni.\n        \"\"\"\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        \"\"\"\n        Calcola e restituisce la media dei valori finora.\n\n        Postcondizioni:\n            - Restituisce la media dei valori finora.\n              Se `iterations` è 0, restituisce 0.\n        \"\"\"\n        print(f'value - self.iterations {self.iterations}, self.current_total {self.current_total}')\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        \"\"\"\n        Reimposta l'accumulo e il numero di iterazioni a zero.\n\n        Postcondizioni:\n            - `current_total` è reimpostato a 0.\n            - `iterations` è reimpostato a 0.\n        \"\"\"\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.343862Z","iopub.execute_input":"2024-01-07T11:55:45.344179Z","iopub.status.idle":"2024-01-07T11:55:45.353793Z","shell.execute_reply.started":"2024-01-07T11:55:45.344144Z","shell.execute_reply":"2024-01-07T11:55:45.353085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DEFINIZIONE DEL MODELLO","metadata":{}},{"cell_type":"code","source":"# Definisci la tua implementazione personalizzata della FasterRCNN\nclass CustomFasterRCNN(FasterRCNN):\n    def __init__(\n        self,\n        backbone,\n        num_classes=None,\n        # transform parameters\n        min_size=800,\n        max_size=1333,\n        image_mean=None,\n        image_std=None,\n        # RPN parameters\n        rpn_anchor_generator=None,\n        rpn_head=None,\n        rpn_pre_nms_top_n_train=2000,\n        rpn_pre_nms_top_n_test=1000,\n        rpn_post_nms_top_n_train=2000,\n        rpn_post_nms_top_n_test=1000,\n        rpn_nms_thresh=0.7,\n        rpn_fg_iou_thresh=0.7,\n        rpn_bg_iou_thresh=0.3,\n        rpn_batch_size_per_image=256,\n        rpn_positive_fraction=0.5,\n        rpn_score_thresh=0.0,\n        # Box parameters\n        box_roi_pool=None,\n        box_head=None,\n        box_predictor=None,\n        box_score_thresh=0.05,\n        box_nms_thresh=0.5,\n        box_detections_per_img=100,\n        box_fg_iou_thresh=0.5,\n        box_bg_iou_thresh=0.5,\n        box_batch_size_per_image=512,\n        box_positive_fraction=0.25,\n        bbox_reg_weights=None,\n        **kwargs,\n    ):\n        # Implementa il tuo costruttore personalizzato\n        super().__init__(\n            backbone, num_classes            \n        )\n        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n        \n        if not hasattr(backbone, \"out_channels\"):\n            raise ValueError(\n                \"backbone should contain an attribute out_channels \"\n                \"specifying the number of output channels (assumed to be the \"\n                \"same for all the levels)\"\n            )\n \n        if not isinstance(rpn_anchor_generator, (AnchorGenerator, type(None))):\n            raise TypeError(\n                f\"rpn_anchor_generator should be of type AnchorGenerator or None instead of {type(rpn_anchor_generator)}\"\n            )\n \n        if num_classes is not None:\n            if box_predictor is not None:\n                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n        else:\n            if box_predictor is None:\n                raise ValueError(\"num_classes should not be None when box_predictor is not specified\")\n \n        out_channels = backbone.out_channels\n \n        if rpn_anchor_generator is None:\n            rpn_anchor_generator = _default_anchorgen()\n        if rpn_head is None:\n            rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n        \n        # Sostituisci la RPN predefinita con la tua implementazione personalizzata\n        self.rpn = CustomRegionProposalNetwork(\n            rpn_anchor_generator,\n            rpn_head,\n            rpn_fg_iou_thresh,\n            rpn_bg_iou_thresh,\n            rpn_batch_size_per_image,\n            rpn_positive_fraction,\n            rpn_pre_nms_top_n,\n            rpn_post_nms_top_n,\n            rpn_nms_thresh,\n            score_thresh=rpn_score_thresh,\n        )\n        \n        '''\n        if box_roi_pool is None:\n            box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)\n\n        if box_head is None:\n            resolution = box_roi_pool.output_size[0]\n            representation_size = 1024\n            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)\n\n        if box_predictor is None:\n            representation_size = 1024\n            box_predictor = FastRCNNPredictor(representation_size, num_classes)\n\n        self.roi_heads = CustomRoIHeads(\n            # Box\n            box_roi_pool,\n            box_head,\n            box_predictor,\n            box_fg_iou_thresh,\n            box_bg_iou_thresh,\n            box_batch_size_per_image,\n            box_positive_fraction,\n            bbox_reg_weights,\n            box_score_thresh,\n            box_nms_thresh,\n            box_detections_per_img,\n        )\n        '''","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.355265Z","iopub.execute_input":"2024-01-07T11:55:45.355553Z","iopub.status.idle":"2024-01-07T11:55:45.370489Z","shell.execute_reply.started":"2024-01-07T11:55:45.355523Z","shell.execute_reply":"2024-01-07T11:55:45.369711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crea il modello Faster R-CNN\nmodel_name = 'resnet50'\nnum_classes = 60\nmodel = get_model(model_name, num_classes)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:45.371771Z","iopub.execute_input":"2024-01-07T11:55:45.372107Z","iopub.status.idle":"2024-01-07T11:55:46.627157Z","shell.execute_reply.started":"2024-01-07T11:55:45.372082Z","shell.execute_reply":"2024-01-07T11:55:46.626156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurazione della trasformazione per l'oggetto model\nmodel.transform = GeneralizedRCNNTransform(\n    min_size=(640,),             # Dimensione minima dell'immagine durante la trasformazione\n    max_size=640,                 # Dimensione massima dell'immagine durante la trasformazione\n    image_mean=[0.485, 0.456, 0.406],   # Media dell'immagine per la normalizzazione\n    image_std=[0.229, 0.224, 0.225]     # Deviazione standard dell'immagine per la normalizzazione\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:46.628424Z","iopub.execute_input":"2024-01-07T11:55:46.628806Z","iopub.status.idle":"2024-01-07T11:55:46.634711Z","shell.execute_reply.started":"2024-01-07T11:55:46.628774Z","shell.execute_reply":"2024-01-07T11:55:46.633855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sposta il modello sulla GPU, se disponibile\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:46.635704Z","iopub.execute_input":"2024-01-07T11:55:46.635984Z","iopub.status.idle":"2024-01-07T11:55:46.891090Z","shell.execute_reply.started":"2024-01-07T11:55:46.635960Z","shell.execute_reply":"2024-01-07T11:55:46.890235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = Averager()     # Accumulatore di loss per train\nval_losses = Averager()       # Accumulatore di loss per la validazione\n\nbest_loss = float('inf')       # Migliore loss inizializzata a infinito\nos.makedirs(weight_path, exist_ok=True)  # Creazione della directory per i pesi del modello, se non esiste già","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:46.892320Z","iopub.execute_input":"2024-01-07T11:55:46.892706Z","iopub.status.idle":"2024-01-07T11:55:46.898576Z","shell.execute_reply.started":"2024-01-07T11:55:46.892674Z","shell.execute_reply":"2024-01-07T11:55:46.897631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IPERPARAMETRI\nnum_epochs = 10\nbatch_size = 16\nnum_workers = 4\nmomentum = 0.9\nlearning_rate = 0.005\n\n# Pesi per la somma pesata delle loss \nrpn_loss_regr_weight = 100     # box regression della region proposal network\nrpn_loss_cls_fixed_num = 1   # object classification della rpn\nclass_loss_cls_weight = 1    # object classfication della parte finale della rete\nclass_loss_regr_weight = 1   # box regression per la parte finale della rete","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:46.899840Z","iopub.execute_input":"2024-01-07T11:55:46.900127Z","iopub.status.idle":"2024-01-07T11:55:46.907561Z","shell.execute_reply.started":"2024-01-07T11:55:46.900103Z","shell.execute_reply":"2024-01-07T11:55:46.906626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definisci i percorsi ai tuoi dataset\ntrain_dataset = BuildingsDataset(train_path, coco_path)\nval_dataset = BuildingsDataset(val_path, coco_path)\ntest_dataset = BuildingsDataset(test_path, coco_path)\n \n# Crea i data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n\n# Definizione dell'ottimizzatore\n#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) \noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-5)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:46.908812Z","iopub.execute_input":"2024-01-07T11:55:46.909225Z","iopub.status.idle":"2024-01-07T11:55:53.796188Z","shell.execute_reply.started":"2024-01-07T11:55:46.909198Z","shell.execute_reply":"2024-01-07T11:55:53.795335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"lossModel_Train = []\nlossModel_Val = []\n        \n# Addestramento\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses.reset()\n    val_losses.reset()\n    \n    # Iterazione su batches di addestramento\n    for batch_index, (images, targets) in enumerate(tqdm(train_loader)):\n        images, targets = [image.to(device) for image in images], [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        model.zero_grad()\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        \n        for key, value in loss_dict.items():\n            if torch.isnan(value):\n                loss_dict[key] = torch.tensor(0.0)\n        \n        # Calcolo della loss totale come combinazione pesata di diverse componenti\n        loss = rpn_loss_regr_weight * loss_dict['loss_rpn_box_reg'] +\\\n               rpn_loss_cls_fixed_num * loss_dict['loss_objectness'] +\\\n               class_loss_cls_weight * loss_dict['loss_classifier'] +\\\n               class_loss_regr_weight * loss_dict['loss_box_reg']\n            \n        # Tracciamento della loss\n        train_losses.send(loss.item())\n\n        # Backpropagation e aggiornamento dei pesi\n        loss.backward()\n        optimizer.step()\n        \n        # Stampa della loss ogni 50 batches\n        '''if batch_index % 50 == 0:\n            print(f\"TRAINING: Epoch: {epoch} Batch Index: {batch_index} Loss: {train_losses.value}\")\n            modelLoss_train = train_losses.value'''\n            \n    modelLoss_train = train_losses.value        \n    print(f\"TRAINING: Epoch: {epoch+1} Loss: {modelLoss_train}\")   \n    \n    # Validazione\n    with torch.no_grad():\n        for batch_index, (images, targets) in enumerate(tqdm(val_loader)):\n            images, targets = [image.to(device) for image in images], [{k: v.to(device) for k, v in t.items()} for t in targets]\n            outputs = model(images, targets)\n  \n            for key, value in outputs.items():\n                if torch.isnan(value):\n                    outputs[key] = torch.tensor(0.0)\n\n            # Calcolo della loss totale durante la validazione\n            loss = rpn_loss_regr_weight * outputs['loss_rpn_box_reg'] +\\\n                    rpn_loss_cls_fixed_num * outputs['loss_objectness'] +\\\n                    class_loss_cls_weight * outputs['loss_classifier'] +\\\n                    class_loss_regr_weight * outputs['loss_box_reg']\n\n            # Tracciamento della loss\n            val_losses.send(loss.item())\n            '''\n            # Stampa della loss di validazione ogni 50 batches\n            if batch_index % 50 == 0:\n                print(f\"VALIDATION: Epoch: {epoch} Batch Index: {batch_index} Loss: {val_losses.value}\")\n                modelLoss_val = val_losses.value \n            '''    \n        modelLoss_val = val_losses.value        \n        print(f\"VALIDATION: Epoch: {epoch+1} Loss: {modelLoss_val}\")\n        \n        # Salvataggio dei migliori pesi se la loss di validazione è migliorata    \n        if (modelLoss_val < best_loss) :\n            print('     .... Saving best weights ....')\n            best_loss = modelLoss_val\n            #salvataggio dei migliori pesi sul validation\n            torch.save(model.state_dict(), weight_path + 'best_model_weights.pth')\n        \n        # Salvataggio delle loss per i plot\n        lossModel_Train.append(modelLoss_train)\n        lossModel_Val.append(modelLoss_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-07T11:55:53.797530Z","iopub.execute_input":"2024-01-07T11:55:53.797929Z","iopub.status.idle":"2024-01-07T14:44:41.617405Z","shell.execute_reply.started":"2024-01-07T11:55:53.797894Z","shell.execute_reply":"2024-01-07T14:44:41.616202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Effettuiamo il plot dele curve di loss\nplt.figure()\nplt.title(\"Model: Training Vs Validation Losses\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(list(range(1,len(lossModel_Train)+1)), lossModel_Train, color='r', label=\"Training Loss\")\nplt.plot(list(range(1, len(lossModel_Val)+1)), lossModel_Val, color='g', label=\"Validation Loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:44:41.619030Z","iopub.execute_input":"2024-01-07T14:44:41.619391Z","iopub.status.idle":"2024-01-07T14:44:42.003687Z","shell.execute_reply.started":"2024-01-07T14:44:41.619361Z","shell.execute_reply":"2024-01-07T14:44:42.002787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test di Prova","metadata":{}},{"cell_type":"code","source":"def visualize_results_with_boxes(image_path, results):\n    bounding_boxes = results['boxes']\n\n    # Carica l'immagine usando PIL\n    image = Image.open(image_path)\n\n    # Inizializza ImageDraw con l'immagine\n    draw = ImageDraw.Draw(image)\n\n    # Disegna i bounding box sull'immagine\n    for bbox in bounding_boxes:\n        bbox_coords = bbox  # Converti la stringa bbox in una lista di coordinate\n        bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[2],bbox_coords[3]]\n        draw.rectangle(bbox_coords, outline='red', width=2)\n\n    # Visualizza l'immagine con i bounding box\n    plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:44:42.005080Z","iopub.execute_input":"2024-01-07T14:44:42.005443Z","iopub.status.idle":"2024-01-07T14:44:42.012334Z","shell.execute_reply.started":"2024-01-07T14:44:42.005410Z","shell.execute_reply":"2024-01-07T14:44:42.011385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_image_with_boxes(image_path, coco_annotations_path):\n    # Carica il file COCO annotations JSON\n    with open(coco_annotations_path, 'r') as coco_file:\n        coco_data = json.load(coco_file)\n\n    # Estrai l'id dell'immagine dal nome del file\n    image_name = image_path.split(\"/\")[-1]\n    image_id = next((img['id'] for img in coco_data['images'] if img['file_name'] == image_name), None)\n\n    # Se l'id dell'immagine è trovato, estrai i bounding box corrispondenti\n    if image_id is not None:\n        bounding_boxes = [bbox for bbox in coco_data['annotations'] if bbox['image_id'] == image_id]\n        \n        # Carica l'immagine usando PIL\n        image = Image.open(image_path)\n\n        # Inizializza ImageDraw con l'immagine\n        draw = ImageDraw.Draw(image)\n\n        # Disegna i bounding box sull'immagine\n        for bbox in bounding_boxes:\n            bbox_coords = eval(bbox['bbox'])  # Converti la stringa bbox in una lista di coordinate\n            bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[0]+bbox_coords[2],bbox_coords[1]+bbox_coords[3]]\n            draw.rectangle(bbox_coords, outline='red', width=2)\n\n        # Visualizza l'immagine con i bounding box\n        plt.imshow(image)\n        plt.show()\n        return image\n    else:\n        print(f\"Image {image_name} not found in COCO annotations.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:44:42.029157Z","iopub.execute_input":"2024-01-07T14:44:42.029522Z","iopub.status.idle":"2024-01-07T14:44:42.038867Z","shell.execute_reply.started":"2024-01-07T14:44:42.029491Z","shell.execute_reply":"2024-01-07T14:44:42.037879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carichiamo i pesi relativi al modello migliore\n#weights = torch.load('/kaggle/input/dataset-ipcv-teama/best_model_weights_rpn_10epoch_16batch_0.005lr.pth')\nweights = torch.load(weight_path + 'best_model_weights.pth')\nmodel.load_state_dict(weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:44:42.040162Z","iopub.execute_input":"2024-01-07T14:44:42.041614Z","iopub.status.idle":"2024-01-07T14:44:42.223605Z","shell.execute_reply.started":"2024-01-07T14:44:42.041580Z","shell.execute_reply":"2024-01-07T14:44:42.222699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = sorted(os.listdir(test_path))\n#print(test_images[0])\n\n# Test su un'immagine\nmodel.eval()\ntest_image = test_dataset[84][0]\n#print(test_image)\n\ntest_image = test_image.to(device)\ntest_image = test_image.unsqueeze(0)\nwith torch.no_grad():\n    output = model(test_image)\n    \nsoglia = 0.1\noutput_tagliato = []\n \nfor detection in output:\n    boxes = detection['boxes']\n    labels = detection['labels']\n    scores = detection['scores']\n \n    # Trova gli indici dei box che superano la soglia\n    indici_superati_soglia = scores >= soglia\n \n    # Filtra i box, le label e gli score in base agli indici superati la soglia\n    boxes_tagliati = boxes[indici_superati_soglia]\n    labels_tagliati = labels[indici_superati_soglia]\n    scores_tagliati = scores[indici_superati_soglia]\n \n    # Creare un nuovo dizionario con i risultati tagliati\n    detection_tagliata = {\n        'boxes': boxes_tagliati,\n        'labels': labels_tagliati,\n        'scores': scores_tagliati\n    }\n \n    # Aggiungere il risultato alla lista finale solo se ci sono box sopra la soglia\n    if len(boxes_tagliati) > 0:\n        output_tagliato.append(detection_tagliata)\n\noriginal_image_path = test_path + '/' + test_images[84]\nimg = visualize_image_with_boxes(original_image_path, coco_path)\nvisualize_results_with_boxes(original_image_path, output_tagliato[0])   ","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:44:42.224841Z","iopub.execute_input":"2024-01-07T14:44:42.225135Z","iopub.status.idle":"2024-01-07T14:44:46.478536Z","shell.execute_reply.started":"2024-01-07T14:44:42.225110Z","shell.execute_reply":"2024-01-07T14:44:46.477257Z"},"trusted":true},"execution_count":null,"outputs":[]}]}