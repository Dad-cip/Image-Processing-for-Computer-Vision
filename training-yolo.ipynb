{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7002615,"sourceType":"datasetVersion","datasetId":4025627},{"sourceId":7044007,"sourceType":"datasetVersion","datasetId":4053100}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# YOLOv8\n\nIn questo notebook, a partire da un dataset precedentemente processato (https://www.kaggle.com/code/angelobarletta/yolo-patches) si realizza l'addestramento della rete YOLOv8. Dopodiché il modello ottenuto e validato viene testato sul set di immagini di test (TestImages)","metadata":{}},{"cell_type":"code","source":"pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:40:52.525252Z","iopub.execute_input":"2024-01-05T14:40:52.525696Z","iopub.status.idle":"2024-01-05T14:41:08.744249Z","shell.execute_reply.started":"2024-01-05T14:40:52.525660Z","shell.execute_reply":"2024-01-05T14:41:08.742854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom io import BytesIO\nimport cv2\nimport matplotlib.pyplot as plt\nimport imageio\nfrom ultralytics import YOLO\nfrom PIL import Image,ImageDraw\nimport zipfile\nimport os\nfrom IPython.display import FileLink\nimport shutil\nimport copy\nfrom tqdm import tqdm\nfrom __future__ import division\nimport scipy.optimize\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:41:09.031529Z","iopub.execute_input":"2024-01-05T14:41:09.031974Z","iopub.status.idle":"2024-01-05T14:41:12.875105Z","shell.execute_reply.started":"2024-01-05T14:41:09.031938Z","shell.execute_reply":"2024-01-05T14:41:12.873965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"# funzione per il download dell'output\ndef zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)\n\n\n# funzione per mostrare l'immagine con i relativi box\ndef show_image_with_boxes(image_path, annotation_file):\n    # Carica l'immagine come array NumPy\n    image_array = imageio.v2.imread(image_path)\n    \n    # Converti l'array NumPy in un oggetto Image\n    image = Image.fromarray(image_array)\n    \n    # Crea un oggetto ImageDraw per disegnare i bounding box sull'immagine\n    draw = ImageDraw.Draw(image)\n    \n    # Dizionario che mappa classi a colori\n    class_colors = {\n        48: \"yellow\",\n        5: \"green\",\n    }\n    default_color = 'red'\n\n    # Leggi le annotazioni dal file di testo\n    with open(annotation_file, 'r') as f:\n        for line in f:\n            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n            \n            # Calcola le coordinate del bounding box in formato assoluto\n            image_width, image_height = image.size\n            x1 = int((x_center - width / 2) * image_width)\n            y1 = int((y_center - height / 2) * image_height)\n            x2 = int((x_center + width / 2) * image_width)\n            y2 = int((y_center + height / 2) * image_height)\n            \n            color = class_colors.get(int(class_id), default_color)\n\n            # Disegna il bounding box sull'immagine\n            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n            draw.text((x1, y1), f\"Class: {int(class_id)}\", fill=color)\n\n    # Mostra l'immagine con i bounding box\n    plt.figure()\n    plt.imshow(image)\n    \n    return image\n\n\n# funzione per mostrare il grafico della loss di train e validation\ndef print_plot(type_loss):\n    \n    # Leggi il CSV\n    df = pd.read_csv(\"/kaggle/working/runs/detect/train/results.csv\")\n\n    df.columns = df.columns.str.strip()   #Elimina spazi prima degli indici\n    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)   #elimina spazi prima dei valori\n\n    # Estrai le colonne necessarie\n    epochs = df['epoch']\n    train_loss = np.float64(df['train/'+type_loss])\n    val_loss = np.float64(df['val/'+type_loss])\n\n    # Crea il grafico\n    plt.figure(figsize=(6, 6))\n    plt.plot(epochs, train_loss, label=type_loss+' (Train)')\n    plt.plot(epochs, val_loss, label=type_loss+' (Val)')\n\n    # Aggiungi etichette e titolo\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Over Epochs')\n    plt.legend()\n\n    # Mostra il grafico\n    plt.show()\n\n    \n# funzione per mostrare il grafico di mAP di train e validation\ndef print_metric(type_metric):\n    \n    # Leggi il CSV\n    df = pd.read_csv(\"/kaggle/working/runs/detect/train/results.csv\")\n\n    df.columns = df.columns.str.strip()   #Elimina spazi prima degli indici\n    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)   #elimina spazi prima dei valori\n\n    # Estrai le colonne necessarie\n    epochs = df['epoch']\n    metric = np.float64(df['metrics/'+type_metric])\n\n    # Crea il grafico\n    plt.figure(figsize=(6, 6))\n    plt.plot(epochs, metric, label=type_metric)\n\n    # Aggiungi etichette e titolo\n    plt.xlabel('Epoch')\n    plt.ylabel(type_metric)\n    plt.title(type_metric+' over epochs')\n    plt.legend()\n\n    # Mostra il grafico\n    plt.show()\n    \n    \n\n# funzione per il match dei box reali e predetti\ndef bbox_iou(boxA, boxB):\n    \n  # Determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    interW = xB - xA\n    interH = yB - yA\n\n    # reject non-overlapping boxes\n    if interW <=0 or interH <=0 :\n        return -1.0\n\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\n\n\ndef match_bboxes(bbox_gt, bbox_pred, IOU_THRESH=0.5):\n    # Given sets of true and predicted bounding-boxes, determine the best possible match.\n    n_true = bbox_gt.shape[0]\n    n_pred = bbox_pred.shape[0]\n    MAX_DIST = 1.0\n    MIN_IOU = 0.0\n\n    # NUM_GT x NUM_PRED\n    iou_matrix = np.zeros((n_true, n_pred))\n    for i in range(n_true):\n        for j in range(n_pred):\n            iou_matrix[i, j] = bbox_iou(bbox_gt[i,1:], bbox_pred[j,1:])\n   \n    # call the Hungarian matching\n    idxs_true, idxs_pred = scipy.optimize.linear_sum_assignment(1 - iou_matrix)\n\n    ious_actual = iou_matrix[idxs_true, idxs_pred]\n    sel_valid = (ious_actual > IOU_THRESH)\n    label = sel_valid.astype(int)\n\n    return idxs_true, idxs_pred, ious_actual, label \n\n\n\n# funzione per convertire le coordinate dal formato [x_center, y_center, width, height] a [x_min, y_min, x_max, y_max]\ndef change_coordinate_format(coordinate_list):\n    for i,coordinate in enumerate(coordinate_list):\n        x_center = coordinate[1]\n        y_center = coordinate[2]\n        width = coordinate[3]\n        heigth = coordinate[4]\n        coordinate[1] = x_center - width/2    # x_min\n        coordinate[2] = y_center - heigth/2   # y_min\n        coordinate[3] = x_center + width/2    # x_max\n        coordinate[4] = y_center + heigth/2   # y_max\n\n\n        \n# calcolo della precision per un'immagine\ndef calculate_precision(tp, fp):\n    # Calcola la precisione\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    return precision\n\n\n\n# calcolo della recall per un'immagine\ndef calculate_recall(tp, fn):\n    # Calcola il recall\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n    return recall\n\n\n\n# calcolo dell'f1 score per un'immagine\ndef calculate_f1_score(precision, recall):\n    # Calcola l'F1 Score\n    f1_score = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) != 0 else 0\n    return f1_score\n\n\n\n# funzione per il calcolo delle metriche per un certo insieme di immagini (test_img_list). \n# Calcola i veri positivi, falsi positivi e falsi negativi per ciascuna immagini, ne misura precision, recall, f1 per ognuna, e infine ne fa la media\ndef calculate_metrics(test_img_list, prediction_path, IOU_THRESH=0.5): \n    tot_true_positive = 0\n    tot_false_positive = 0\n    tot_false_negative = 0\n    \n    precision = 0\n    recall = 0\n    f1 = 0\n\n    #prediction_path = '/kaggle/working/runs/detect/predict6/labels'\n    label_path = '/kaggle/input/xview-dataset-team1/xview-dataset-team1/labels'\n    \n    # accede alle singole immagini sulle quali è stat fatta predizione\n    for elem in test_img_list:\n        \n        true_positive = 0\n        false_positive = 0\n        false_negative = 0\n        \n        elem = elem.split('.')[0]\n        elem = elem + '.txt'\n        pred_img_path = os.path.join(prediction_path, elem)\n        label_img_path = os.path.join(label_path, elem)\n        \n        if not os.path.exists(pred_img_path):     # se non esite il file delle predizioni, tutti i box della groud truth saranno falsi negativi\n            with open(label_img_path, 'r') as file:\n                false_negative = len(file.readlines())\n                #print(f'TP:{true_positive}, FP:{false_positive}, FN:{false_negative}')\n                \n        else:   # se essite il file delle predizioni allora bisogna confrontare i box con quelli della ground truth\n            \n            with open(pred_img_path, 'r') as boxfile:\n                boxes_pred = [line.strip() for line in boxfile]\n                boxes_pred = [list(map(float, stringa.split())) for stringa in boxes_pred]\n                boxes_pred = np.vstack(boxes_pred)\n                \n            with open(label_img_path, 'r') as boxfile:\n                boxes_label = [line.strip() for line in boxfile]\n                boxes_label = [list(map(float, stringa.split())) for stringa in boxes_label]\n                if len(boxes_label) > 0:\n                    boxes_label = np.vstack(boxes_label)\n                \n            change_coordinate_format(boxes_pred)\n            change_coordinate_format(boxes_label)\n\n            match_result = match_bboxes(boxes_label, boxes_pred, IOU_THRESH)   #treshold è il valore di iou oltre il quale c'è un match tra i box (di default è 0.7)\n            #print(f'{match_result}\\n')\n            \n            # calcolo falsi negativi e falsi positivi\n            if boxes_label.shape[0] > boxes_pred.shape[0]:     # ho più box reali che predetti\n                false_negative = boxes_label.shape[0] - boxes_pred.shape[0]\n            elif boxes_label.shape[0] < boxes_pred.shape[0]:   # ho più box predetti che reali\n                false_positive = boxes_pred.shape[0] - boxes_label.shape[0]\n            \n            for i in range(len(match_result[0])):\n                #print(f'gt:{match_result[0][i]}, pred:{match_result[1][i]}, iou:{match_result[2][i]}, label:{match_result[3][i]}')\n            \n                index_gt = match_result[0][i]\n                index_pred = match_result[1][i]\n                if (boxes_label[index_gt,0] == boxes_pred[index_pred,0]) and (match_result[3][i] == 1):\n                    true_positive += 1\n                else:\n                    false_positive += 1\n                    false_negative += 1\n\n            #print(f'TP:{true_positive}, FP:{false_positive}, FN:{false_negative}')\n       \n        # calcolo metriche\n        precision += calculate_precision(true_positive, false_positive)\n        recall += calculate_recall(true_positive, false_negative)\n        f1 += calculate_f1_score(calculate_precision(true_positive, false_positive), calculate_recall(true_positive, false_negative))\n    \n        tot_true_positive += true_positive\n        tot_false_positive += false_positive\n        tot_false_negative += false_negative\n        \n    precision = precision/len(test_img_list)\n    recall = recall/len(test_img_list)\n    f1 = calculate_f1_score(precision, recall)\n    f1 = f1/len(test_img_list)\n    \n    print(f'precision:{precision}, recall:{recall}, f1_score:{f1}')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:41:12.877292Z","iopub.execute_input":"2024-01-05T14:41:12.877899Z","iopub.status.idle":"2024-01-05T14:41:12.928628Z","shell.execute_reply.started":"2024-01-05T14:41:12.877858Z","shell.execute_reply":"2024-01-05T14:41:12.927522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training + Validation","metadata":{}},{"cell_type":"markdown","source":"Caricamento modello preaddestrato e fine tuning.  \nPer l'addestramento della rete gli viene dato in input il file di configurazione <b>*xview_yolo.yaml*</b>, contentente le informazioni su dove recuperare le immagini di train e validation.","metadata":{}},{"cell_type":"code","source":"# caricamento del modello preaddestrato\nmodel = YOLO('yolov8s.pt')\n\n# addestramento del modello sul dataset xview\nresults = model.train(data='/kaggle/input/xview-dataset-team1/xview-dataset-team1/YOLO_cfg/xview_yolo.yaml', epochs=100, imgsz=640, optimizer='Adam', lr0=0.001, name='train')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:11:38.379243Z","iopub.execute_input":"2024-01-04T13:11:38.380192Z","iopub.status.idle":"2024-01-04T23:56:45.550674Z","shell.execute_reply.started":"2024-01-04T13:11:38.380149Z","shell.execute_reply":"2024-01-04T23:56:45.549505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Caricamento del modello con i pesi migliori","metadata":{}},{"cell_type":"code","source":"model_test = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:15:56.804786Z","iopub.execute_input":"2024-01-05T10:15:56.805297Z","iopub.status.idle":"2024-01-05T10:15:57.136821Z","shell.execute_reply.started":"2024-01-05T10:15:56.805258Z","shell.execute_reply":"2024-01-05T10:15:57.135575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stampa delle loss e della mean average precision di train e validation","metadata":{}},{"cell_type":"code","source":"print_plot('box_loss')\nprint_plot('cls_loss')\nprint_metric('mAP50(B)')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:16:57.109498Z","iopub.execute_input":"2024-01-05T10:16:57.109912Z","iopub.status.idle":"2024-01-05T10:16:58.129098Z","shell.execute_reply.started":"2024-01-05T10:16:57.109881Z","shell.execute_reply":"2024-01-05T10:16:58.124936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing (labellato)\nProcediamo ora a testare il modello su un set di immagini di test ricavate dal dataset di partenza, e delle quali conosciamo quindi le ground truth","metadata":{}},{"cell_type":"markdown","source":"#### Definiamo una serie di funzioni per dividere le immagini di test in tanti crop e rimettere insieme i risultati","metadata":{}},{"cell_type":"markdown","source":"Questa funzione calcola a partire da un'immagine le coordinate dei crop che dovranno essere realizzati.  \nI crop possono essere realizzati con sovrapposizione per evitare di non riuscire a predirre oggetti troncati dai crop.","metadata":{}},{"cell_type":"code","source":"def calculate_crop(original_width, original_heigth, scarto_sovrapposizione=200):\n    #Definizione lista di coordinate per i crop delle immagini (lungo la direzione x)\n    dim_x = original_width\n    x_init = 0\n    x_end = 640\n    lista_x = []\n\n    while (x_end <= dim_x):\n        lista_x.append([x_init, x_end])\n        x_init = x_end - scarto_sovrapposizione\n        x_end = x_init + 640\n\n    if (x_end > dim_x):\n        lista_x.append([x_init, dim_x])\n\n    #Definizione lista di coordinate per i crop delle immagini (lungo la direzione y)\n    dim_y = original_heigth\n    y_init = 0\n    y_end = 640\n    scarto_sovrapposizione = 200\n    lista_y = []\n\n    while (y_end <= dim_y):\n        lista_y.append([y_init, y_end])\n        y_init = y_end - scarto_sovrapposizione\n        y_end = y_init + 640\n\n    if (y_end > dim_y):\n        lista_y.append([y_init, dim_y])\n    \n    return lista_x, lista_y","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:17:42.494345Z","iopub.execute_input":"2024-01-05T10:17:42.494819Z","iopub.status.idle":"2024-01-05T10:17:42.504382Z","shell.execute_reply.started":"2024-01-05T10:17:42.494774Z","shell.execute_reply":"2024-01-05T10:17:42.503222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Questa funzione, a partire dalle liste recedentemente calcolate, esegue il crop delle immagini, salvandole nel path specificato (*crop_img_path*).  \nPer assicurarsi che ciascun crop rispetti le dimensioni fissate di (640x640) viene effettuato un padding dove necessario.","metadata":{}},{"cell_type":"code","source":"def create_crop(img, lista_x, lista_y, crop_img_path):\n    lista_img = []\n\n    for elem_y in lista_y:\n        for elem_x in lista_x:\n            dim_x = elem_x[1] - elem_x[0]\n            dim_y = elem_y[1] - elem_y[0]\n            if (dim_x == 640) and (dim_y == 640) :\n                lista_img.append(img[elem_y[0]:elem_y[1], elem_x[0]:elem_x[1], :])\n            else:\n                temp = img[elem_y[0]:elem_y[1], elem_x[0]:elem_x[1], :]\n                if (dim_x != 640):\n                    padding = np.zeros((dim_y, 640-dim_x, 3), dtype=np.uint8)\n                    temp = np.concatenate((temp,padding), axis=1)     #concatena l'immagine temp e gli zeri lungo la direzione orizzontale\n                    dim_x = 640\n                if (dim_y != 640):\n                    padding = np.zeros((640-dim_y, dim_x, 3), dtype=np.uint8)\n                    temp = np.concatenate((temp,padding), axis=0)     #concatena l'immagine temp e gli zeri lungo la direzione verticale\n                    dim_y = 640\n                lista_img.append(temp)\n\n    # salva i crop nel percorso specificato\n    for i, img_chunk in enumerate(lista_img):\n        save_path = crop_img_path + 'image_' + str(i) + '.jpg'   #l'indice \"i\" serve per mantenere l'ordine dei crop\n        plt.imsave(save_path, img_chunk)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:17:46.825928Z","iopub.execute_input":"2024-01-05T10:17:46.826736Z","iopub.status.idle":"2024-01-05T10:17:46.840813Z","shell.execute_reply.started":"2024-01-05T10:17:46.826660Z","shell.execute_reply":"2024-01-05T10:17:46.839406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Questa funzione può eventualmente essere usata per ricostruire l'immagine di partenza a partire dai suoi crop e dalle coordinate di ciascuno di essi","metadata":{}},{"cell_type":"code","source":"def reconstruct_image(crop_img_path, posizioni):\n    lista_img = os.listdir(crop_img_path)\n\n    original_img = Image.new('RGB', (original_width, original_heigth), color='white')\n\n    for img in os.listdir(crop_img_path):\n        img_index = img.split('.')[0]\n        img_index = int(img_index.split('_')[-1])\n        img = Image.open(crop_img_path + img)\n        original_img.paste(img, posizioni[img_index])\n\n    original_img = np.array(original_img)\n    plt.figure()\n    plt.imshow(original_img)\n    \n    return original_img","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:17:47.491303Z","iopub.execute_input":"2024-01-05T10:17:47.491746Z","iopub.status.idle":"2024-01-05T10:17:47.500377Z","shell.execute_reply.started":"2024-01-05T10:17:47.491710Z","shell.execute_reply":"2024-01-05T10:17:47.498897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Questa funzione permette di raggruppare le predizioni dei crop di una stessa immagine in un unico file, questo file sarà la predizione sull'immagine originale.","metadata":{}},{"cell_type":"code","source":"def aggregate_label(crop_img_path, crop_label_path, output_file_path, posizioni, original_width, original_heigth):\n    # qui va a prendere il crop creato, e ne calcola le dimensioni, questo serve per la denormalizzazione delle coordinate dei box\n    for elem in os.listdir(crop_label_path):\n        img_name = elem.split('.txt')[0]\n        img_path = crop_img_path + img_name + '.jpg'\n        img = plt.imread(img_path)\n        heigth, width, band = img.shape\n\n        with open(crop_label_path+elem, 'r') as file:\n            boxes_label = [line.strip() for line in file]\n            boxes_label = [list(map(float, stringa.split())) for stringa in boxes_label]\n\n            # associa a ciascun box il crop sul quale è calcolato, quindi in base alla posizione del crop vengono adattate le coordinate\n            index = int(img_name.split('_')[-1])\n            with open(output_file_path, 'a') as file:\n                for line in boxes_label:\n                    # denormalizzazione coordinate\n                    x_center = line[1] * width\n                    y_center = line[2] * heigth\n                    box_width = line[3] * width\n                    box_heigth = line[4] * heigth\n                    # shift + normalizzazione rispetto alle dimensioni dell'immagine originale\n                    line[1] = (x_center + posizioni[index][0]) / original_width\n                    line[2] = (y_center + posizioni[index][1]) / original_heigth\n                    line[3] = box_width / original_width\n                    line[4] = box_heigth / original_heigth\n\n                    # scriviamo nel file contentente i box anche l'indice del crop dal quale proviene, ci servirà dopo per la rimozione dei box ridondanti\n                    file.write(f\"{line[0]} {line[1]} {line[2]} {line[3]} {line[4]} {index}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:31:13.892872Z","iopub.execute_input":"2024-01-05T10:31:13.893336Z","iopub.status.idle":"2024-01-05T10:31:13.907047Z","shell.execute_reply.started":"2024-01-05T10:31:13.893300Z","shell.execute_reply":"2024-01-05T10:31:13.905567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Questa funzione ci permette di eliminare i box che risultano sovrapposti oltre una certa soglia.  \nQuesto si rende necessario dal momento che i crop sono eseguiti con sovrapposizione e ci sono quindi delle regioni comuni tra essi, che possono far si che uno stesso box sia predetto più volte.","metadata":{}},{"cell_type":"code","source":"def remove_overlapped_boxes(label_path, treshold=0.25):\n    indici_da_eliminare = []\n\n    with open (label_path, 'r') as file:\n        boxes = [line.strip() for line in file]\n        boxes = [list(map(float, stringa.split())) for stringa in boxes]\n        \n        #print(boxes[0])\n\n    # boxes_temp contiene i box con le coordinate convertite nel formato: (label,xmin,ymin,xmax,ymax)\n    boxes_temp = copy.deepcopy(boxes)\n    change_coordinate_format(boxes_temp)\n\n    # alcuni box si sovrappongono perché i crop sono con sovrapposizione, quindi applichiamo una sorta di non maximal suppression\n    # confrontiamo ciascun box con tutti gli altri della llista (escluso se stesso)\n    # se due box appartenenti a crop diversi sono della stessa classe e il valore di sovrapposizione supera una certa soglia (calcolata con iou) allora il più piccolo dei due viene eliminato\n    for i,box in tqdm(enumerate(boxes_temp)):\n        for j,box_2 in enumerate(boxes_temp):\n            if j != i:\n                \n                # confrontiamo i box da eliminare solo se provengono da crop diversi (index aggiunto prima), per eliminare solo i box dovuti alle sovrapposizioni dei crop (uso un basso valore di treshold)\n                if box[5] != box_2[5]:      \n                    \n                    if box[0] == box_2[0]:\n                        iou = bbox_iou(box[1:], box_2[1:])\n                        if iou >= treshold:\n                            area = (box[3] - box[1]) * (box[4] - box[2])    #area = (xmax-xmin)*(ymax-ymin)\n                            area_2 = (box_2[3] - box_2[1]) * (box_2[4] - box_2[2])\n                            if area_2 <= area:\n                                indici_da_eliminare.append(j)\n                                \n                else:\n                    # cerco di eliminare solo i box fortemente sovrapposti (treshold alto) dovuti ad una detection poco precisa del modello \n                    pass\n\n    # la lista di indici è ordinata in modo decrescente, per evitare che durante l'eliminazione ci siano incongruenze tra gli indici\n    # (eliminando in ordine crescente, il valore degli indici va a scalare e quindi non si trovano più)\n    indici_da_eliminare = list(set(indici_da_eliminare))\n    indici_da_eliminare.sort(reverse=True)\n    for elem in indici_da_eliminare:\n        del boxes[elem]\n\n    # riscriviamo il file dei box,che conterrà ora solo i box non sovrapposti\n    with open(label_path, 'w') as file:\n        for line in boxes:\n            file.write(f\"{line[0]} {line[1]} {line[2]} {line[3]} {line[4]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:31:16.967376Z","iopub.execute_input":"2024-01-05T10:31:16.967804Z","iopub.status.idle":"2024-01-05T10:31:16.982362Z","shell.execute_reply.started":"2024-01-05T10:31:16.967773Z","shell.execute_reply":"2024-01-05T10:31:16.980978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definizione della lista di immagini di test sulle quali testiamo il modello.  \nQueste immagini sono a dimensione originale. Per fare detection il modello utilizzerà le funzioni precedentemente definite per creare i crop, fare previsioni su di essi, e rimettere poi insieme i risultati per ottenere la predizione sull'immagine di partenza.","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/xview-dataset-team1/xview-dataset-team1/YOLO_cfg/test.txt'\n\nwith open(file_path, 'r') as file:\n    # Leggi tutte le righe del test.txt\n    test_img_list = [line.strip() for line in file.readlines()]\n\nfor i,line in enumerate(test_img_list):\n    test_img_list[i] = line.split('/')[-1]    # prendo il nome dell'immagine con l'estensione .jpg","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:31:20.580401Z","iopub.execute_input":"2024-01-05T10:31:20.580884Z","iopub.status.idle":"2024-01-05T10:31:20.595554Z","shell.execute_reply.started":"2024-01-05T10:31:20.580821Z","shell.execute_reply":"2024-01-05T10:31:20.594281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"local_test_path = '/kaggle/input/xview-dataset-team1/xview-dataset-team1/images/'\nlocal_crop_img_path = '/kaggle/working/local_crop_image/'\ncrop_pred_path = '/kaggle/working/runs/detect/predict/'\ncrop_label_path = '/kaggle/working/runs/detect/predict/labels/'\noutput_file_path = '/kaggle/working/local_test_predictions/'\n\nif not os.path.exists(output_file_path):\n    os.mkdir(output_file_path)\n\nif not os.path.exists(local_crop_img_path):\n    os.mkdir(local_crop_img_path)\n    \ni=0\n\nfor elem in test_img_list:\n    \n    print(f'\\ni:{i}')\n    i += 1\n    \n    # apriamo l'immagine per ottenere la lunghezza e la larghezza\n    img = plt.imread(local_test_path + elem)\n    original_heigth, original_width, _ = img.shape\n    # calcolo delle coordinate dei crop\n    lista_x, lista_y = calculate_crop(original_width, original_heigth, scarto_sovrapposizione=200)\n    \n    # creazione di una directory che conterrà tutti i crop per l'immagine corrente\n    crop_save_path = local_crop_img_path + elem.split('.')[0] + '/'\n    if not os.path.exists(crop_save_path):\n        os.mkdir(crop_save_path)\n        # creazione dei crop per l'immagine corrente\n        create_crop(img, lista_x, lista_y, crop_save_path)\n    \n    print(f'crop creati per {elem}')\n    \n    # predizione su tutti i crop dell'immagine corrente (sono contenuti in crop_save_path)\n    temp = model_test.predict(crop_save_path, save_txt=True, verbose=False, name='predict')\n    \n    # viene creato un vettore di posizioni, che tiene conto della posizione di ciascun crop rispetto all'immagine originale (as es. (0,0), (0,640), (640,640), ...)\n    # servono per adattare (ovvero traslare) i box calcolati sui crop all'immagine originale (quella grande)\n    posizioni = []\n    for elem_y in lista_y:\n        for elem_x in lista_x:\n            posizioni.append((elem_x[0],elem_y[0]))\n            \n    print(f'posizioni calcolate per {elem}')\n    \n    # i box dei diversi crop vengono uniti in un unico file, questi box vengono anche adattati all'immagine originale tramite traslazione e normalizzazione\n    test_label_path = output_file_path + elem.split('.')[0] + '.txt'   # nome del file col quale sarà salvata la label di questa immagine\n    aggregate_label(crop_save_path, crop_label_path, test_label_path, posizioni, original_width, original_heigth)\n    print(f'label aggregate per {elem}')\n    if os.path.exists(test_label_path):\n        # vengono rimossi i box sovrapposti\n        remove_overlapped_boxes(test_label_path, treshold=0.25)\n        print(f'box rimossi per {elem}')\n    \n    \n    shutil.rmtree(crop_pred_path)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calcolo delle metriche di recall, precisione ed f1_score","metadata":{}},{"cell_type":"code","source":"calculate_metrics(test_img_list, output_file_path, IOU_THRESH=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:11:50.440983Z","iopub.execute_input":"2024-01-05T11:11:50.441543Z","iopub.status.idle":"2024-01-05T11:22:15.517041Z","shell.execute_reply.started":"2024-01-05T11:11:50.441504Z","shell.execute_reply":"2024-01-05T11:22:15.515420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing (non labellato)\nAndiamo ora a testare il modello sul test set, quindi senza label.  ","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages/'\ncrop_img_path = '/kaggle/working/crop_image/'\ncrop_pred_path = '/kaggle/working/runs/detect/predict/'\ncrop_label_path = '/kaggle/working/runs/detect/predict/labels/'\noutput_file_path = '/kaggle/working/test_predictions/'\n\nif not os.path.exists(output_file_path):\n    os.mkdir(output_file_path)\n\nif not os.path.exists(crop_img_path):\n    os.mkdir(crop_img_path)\n    \ni=0\n    \n    \nfor elem in os.listdir(test_path):\n    \n    print(f'\\ni:{i}')\n    i += 1\n    \n    # apriamo l'immagine per ottenere la lunghezza e la larghezza\n    img = plt.imread(test_path + elem)\n    original_heigth, original_width, _ = img.shape\n    lista_x, lista_y = calculate_crop(original_width, original_heigth)\n    \n    # creazione dei crop per la singola immagine\n    crop_save_path = crop_img_path + elem.split('.')[0] + '/'\n    if not os.path.exists(crop_save_path):\n        os.mkdir(crop_save_path)\n        create_crop(img, lista_x, lista_y, crop_save_path)\n    \n    print(f'crop creati per {elem}')\n    \n    temp = model_test.predict(crop_save_path, save_txt=True, verbose=False, name='predict')\n    \n    # viene creato un vettore di posizioni, che tiene conto della posizione di ciascun crop rispetto all'immagine originale (as es. (0,0), (0,640), (640,640), ...)\n    # servono per adattare (ovvero spostare) i box calcolati sui crop all'immagine originale (quella grande)\n    posizioni = []\n    for y,elem_y in enumerate(lista_y):\n        for elem_x in lista_x:\n            posizioni.append((elem_x[0],elem_y[0]))\n            \n    print(f'posizioni calcolate per {elem}')\n    \n    # i box dei diversi crop vengono uniti in un unico file, questi box vengono anche adattati all'immagine originale tramite shift e normalizzazione\n    test_label_path = output_file_path + elem.split('.')[0] + '.txt'\n    aggregate_label(crop_save_path, crop_label_path, test_label_path, posizioni, original_width, original_heigth)\n    print(f'label aggregate per {elem}')\n    if os.path.exists(test_label_path):\n        # vengono rimossi i box sovrapposti\n        remove_overlapped_boxes(test_label_path, treshold=0.25)\n        print(f'box rimossi per {elem}')\n    \n    \n    shutil.rmtree(crop_pred_path)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mostro i risultati su alcune immagini di test","metadata":{}},{"cell_type":"code","source":"img = show_image_with_boxes('/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages/367.jpg', '/kaggle/working/test_predictions/367.txt')\n#img.save('/kaggle/working/test_img.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = show_image_with_boxes('/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages/1464.jpg', '/kaggle/working/test_predictions/1464.txt')\n#img.save('/kaggle/working/test_img.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = show_image_with_boxes('/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages/2077.jpg', '/kaggle/working/test_predictions/2077.txt')\n#img.save('/kaggle/working/test_img.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = show_image_with_boxes('/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages/178.jpg', '/kaggle/working/test_predictions/178.txt')\n#img.save('/kaggle/working/test_img.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shutil.rmtree('/kaggle/working/local_test_predictions')\n#shutil.rmtree('/kaggle/working/local_crop_image')\n#shutil.rmtree('/kaggle/working/test_predictions')\n#shutil.rmtree('/kaggle/working/crop_image')\n#shutil.rmtree('/kaggle/working/runs/detect/predict')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:31:02.954680Z","iopub.execute_input":"2024-01-05T10:31:02.955171Z","iopub.status.idle":"2024-01-05T10:31:02.965728Z","shell.execute_reply.started":"2024-01-05T10:31:02.955137Z","shell.execute_reply":"2024-01-05T10:31:02.964694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}