{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7308016,"sourceType":"datasetVersion","datasetId":4212640}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:41:55.584026Z","iopub.execute_input":"2024-01-01T17:41:55.584903Z","iopub.status.idle":"2024-01-01T17:42:08.711504Z","shell.execute_reply.started":"2024-01-01T17:41:55.584868Z","shell.execute_reply":"2024-01-01T17:42:08.710408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np                                    # Importa Numpy\nimport skimage.io as io                               # Importa il modulo Input/ouput di SK-Image\nfrom skimage.transform import resize                  # Importa il modulo resize da SK-Image\nfrom os import listdir                                # Importa il modulo listdir da OS\n\nimport json                                           # Importa Json\nfrom matplotlib.collections import PatchCollection    # Importa PatchCollection dal modulo collections di MatPlotLib\nfrom pycocotools.coco import COCO                     # Importa COCO dal modulo coco di PyCoco-Tools\nimport pycocotools.mask as cocomask                   # Importa il modulo Mask di PyCoco-Tools\nimport matplotlib.pyplot as plt                       # Importa il modulo  pyplot di MatPlotLib\n\nimport PIL\nfrom PIL import Image, ImageDraw                      # Importa il modulo Image da PIL\n\nfrom tensorflow import keras                          # Importa il modulo Keras di TensorFlow\nimport os                                             # Importa os\n\nfrom tqdm import tqdm                                 # Importa il modulo tqdm da tqdm\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.transforms import transforms\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\n\nimport torch.nn.functional as F\n\nimport cv2\n\nimport scipy.optimize\nimport copy\nimport shutil\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:08.713792Z","iopub.execute_input":"2024-01-01T17:42:08.714095Z","iopub.status.idle":"2024-01-01T17:42:22.734515Z","shell.execute_reply.started":"2024-01-01T17:42:08.714061Z","shell.execute_reply":"2024-01-01T17:42:22.733614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path\npath_in = '/kaggle/input/dataset-ipcv-teama/'\npath_out = '/kaggle/working/'\n\ncoco_path = path_in + 'COCO_annotations.json/COCO_annotations.json'\ntrain_path = path_in + 'dataset_IPCV/kaggle/working/train'\ntest_path = path_in + 'dataset_IPCV/kaggle/working/test'\nval_path = path_in + 'dataset_IPCV/kaggle/working/val'\n\nweight_path = path_out + 'weight/'\n\n# Path delle predizioni locali\nlocal_crop_img_path = path_out + 'local_crop_image/' # path dei crop delle immagini di test\ncrop_pred_path = path_out + 'predict/' # path delle predizioni sui crop\noutput_file_path = path_out + 'local_test_predictions/' # path di salvataggio delle predizioni sui crop\nlabel_path = path_out + 'labels/'\n\n# Path delle predizioni senza labels\nnolabels_test_path = '/kaggle/input/xview-dataset-team1/xview-dataset-team1/TestImages'\ncrop_img_path = path_out + 'crop_image/'\nnolabels_output_file_path = path_out + 'test_predictions/'","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.735663Z","iopub.execute_input":"2024-01-01T17:42:22.736201Z","iopub.status.idle":"2024-01-01T17:42:22.742427Z","shell.execute_reply.started":"2024-01-01T17:42:22.736174Z","shell.execute_reply":"2024-01-01T17:42:22.741405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Def. funzioni","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n    \"\"\"\n    Carica un'immagine da un percorso specificato.\n\n    Precondizione:\n        - `image_path` è il percorso completo dell'immagine da caricare.\n\n    Postcondizione:\n        - Restituisce l'immagine caricata.\n        - Solleva un'eccezione AssertionError se l'immagine non viene trovata nel percorso specificato.\n    \"\"\"\n    image = plt.imread(image_path)\n    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.745036Z","iopub.execute_input":"2024-01-01T17:42:22.746180Z","iopub.status.idle":"2024-01-01T17:42:22.781945Z","shell.execute_reply.started":"2024-01-01T17:42:22.746115Z","shell.execute_reply":"2024-01-01T17:42:22.781049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BuildingsDataset(Dataset):\n    def __init__(self, images_dir, annotation_path=None, transform=None):\n        \"\"\"\n        Inizializza un oggetto BuildingsDataset.\n\n        Precondizioni:\n            - `images_dir` è il percorso della directory contenente le immagini.\n            - `annotation_path` è il percorso del file JSON contenente le annotazioni.\n            - `transform` è un oggetto trasformazione (es. da torchvision.transforms) per applicare\n              trasformazioni alle immagini e alle etichette, opzionale.\n\n        Postcondizioni:\n            - `image_paths` è una lista di percorsi completi delle immagini nella directory `images_dir`.\n            - `annotations` contiene le annotazioni caricate dal file JSON specificato.\n            - `transform` è l'oggetto trasformazione fornito.\n        \"\"\"\n        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n        self.transform = transform\n        self.annotation_path = annotation_path\n \n        if self.annotation_path is not None:\n            with open(annotation_path, 'r') as f:\n                self.annotations = json.load(f)\n \n    def transform_image_bbox(self, image, label):\n        \"\"\"\n        Applica trasformazioni all'immagine e alle etichette, se definite.\n\n        Precondizioni:\n            - `image` è l'immagine da trasformare.\n            - `label` sono le etichette associate all'immagine.\n\n        Postcondizioni:\n            - Se `transform` è definito, applica le trasformazioni all'immagine e alle etichette.\n            - Restituisce l'immagine e le etichette trasformate.\n        \"\"\"\n        if self.transform:\n            transformed = self.transform(image=image, labels=label)\n            image = transformed['image']\n            label = transformed['labels']\n \n        image = transforms.ToTensor()(image)\n        return image, label\n \n    def __getitem__(self, index):\n        \"\"\"\n        Restituisce un campione specifico dell'insieme di dati.\n\n        Precondizioni:\n            - `index` è l'indice del campione da restituire.\n\n        Postcondizioni:\n            - Restituisce un'immagine e le relative etichette nel formato richiesto.\n              Se non ci sono annotazioni per l'immagine, restituisce un target vuoto.\n        \"\"\"\n        image_path = self.image_paths[index]\n        image_name = image_path.split(\"/\")[-1]\n        image = load_image(image_path).astype(np.float32)\n        image /= 255.0\n        image = torch.from_numpy(image).permute(2,0,1) # change the shape from [h,w,c] to [c,h,w]  \n\n        if self.annotation_path is not None:\n            image_id = next((img['id'] for img in self.annotations['images'] if img['file_name'] == image_name), None)\n            box_lab = [anno for anno in self.annotations['annotations'] if anno['image_id'] == image_id]\n\n            if(len(box_lab) == 0):\n                target = {}\n                boxes = torch.zeros((0, 4), dtype=torch.float32) \n                target = {\n                        \"boxes\": boxes, \n                        \"labels\": torch.zeros(0, dtype=torch.int64), \n                        \"image_id\": torch.as_tensor([4])\n                         }\n            else:\n                boxes = [json.loads(record['bbox']) for record in box_lab]\n                categories = [record['category_id'] for record in box_lab]\n                boxes = np.array(boxes)\n                # change the co-ordinates into expected [x, y, x+w, y+h] format\n                boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n                boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n                labels = torch.as_tensor(categories, dtype=torch.int64)\n\n                target = {\n                       \"boxes\": boxes, \n                       \"labels\": labels,\n                       \"image_id\": torch.tensor([index])\n                        }\n\n            return image, target\n        else:\n            return image, None\n        \n \n    def __len__(self):\n        \"\"\"\n        Restituisce il numero totale di campioni nell'insieme di dati.\n\n        Postcondizioni:\n            - Restituisce la lunghezza dell'insieme di dati.\n        \"\"\"\n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.783359Z","iopub.execute_input":"2024-01-01T17:42:22.783779Z","iopub.status.idle":"2024-01-01T17:42:22.801544Z","shell.execute_reply.started":"2024-01-01T17:42:22.783754Z","shell.execute_reply":"2024-01-01T17:42:22.800620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name='resnet50', num_classes=60):\n    \"\"\"\n    Restituisce un modello Faster R-CNN con una determinata architettura di backbone.\n\n    Precondizioni:\n        - `model_name` è una stringa che specifica l'architettura del backbone. \n          Default è 'resnet50'.\n        - `num_classes` è il numero di classi dell'insieme di dati. Default è 60.\n\n    Postcondizioni:\n        - Restituisce un modello Faster R-CNN con il backbone specificato e il numero di classi.\n    \"\"\"\n    backbone = resnet_fpn_backbone(model_name, pretrained=True)\n    model = FasterRCNN(backbone, num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.802680Z","iopub.execute_input":"2024-01-01T17:42:22.803004Z","iopub.status.idle":"2024-01-01T17:42:22.816286Z","shell.execute_reply.started":"2024-01-01T17:42:22.802974Z","shell.execute_reply":"2024-01-01T17:42:22.815608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Funzione di aggregazione per un batch di campioni.\n\n    Precondizioni:\n        - `batch` è una lista di campioni, ciascuno nel formato (immagine, target).\n          Dove 'immagine' è un tensore rappresentante l'immagine e 'target' è un dizionario\n          contenente le etichette associate all'immagine.\n\n    Postcondizioni:\n        - Restituisce una tupla di due elementi:\n          1. Un tensore contenente tutte le immagini del batch.\n          2. Una lista di dizionari rappresentanti i target corrispondenti alle immagini.\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.817331Z","iopub.execute_input":"2024-01-01T17:42:22.817629Z","iopub.status.idle":"2024-01-01T17:42:22.829873Z","shell.execute_reply.started":"2024-01-01T17:42:22.817605Z","shell.execute_reply":"2024-01-01T17:42:22.828989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_ap(precision, recall):\n    # Calcola l'area sotto la curva Precision-Recall (AP)\n    ap = 0\n    for i in range(1, len(precision)):\n        ap += (recall[i] - recall[i-1]) * precision[i]\n    return ap\n\n\ndef calculate_map(ap_list):\n    # Calcola la media delle AP\n    mAP = sum(ap_list) / len(ap_list)\n    return mAP\n\n\ndef calculate_precision(tp, fp):\n    # Calcola la precisione\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    return precision\n\n\ndef calculate_recall(tp, fn):\n    # Calcola il recall\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n    return recall\n\n\ndef calculate_f1_score(precision, recall):\n    # Calcola l'F1 Score\n    f1_score = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) != 0 else 0\n    return f1_score","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.831143Z","iopub.execute_input":"2024-01-01T17:42:22.831418Z","iopub.status.idle":"2024-01-01T17:42:22.839322Z","shell.execute_reply.started":"2024-01-01T17:42:22.831372Z","shell.execute_reply":"2024-01-01T17:42:22.838535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bbox_iou(boxA, boxB):\n    \n  # Determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    interW = xB - xA\n    interH = yB - yA\n\n    # reject non-overlapping boxes\n    if interW <=0 or interH <=0 :\n        return -1.0\n\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.840314Z","iopub.execute_input":"2024-01-01T17:42:22.840637Z","iopub.status.idle":"2024-01-01T17:42:22.850490Z","shell.execute_reply.started":"2024-01-01T17:42:22.840614Z","shell.execute_reply":"2024-01-01T17:42:22.849731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_bboxes(bbox_gt, bbox_pred, IOU_THRESH=0.5):\n    # Given sets of true and predicted bounding-boxes, determine the best possible match.\n    n_true = bbox_gt.shape[0]\n    n_pred = bbox_pred.shape[0]\n    MAX_DIST = 1.0\n    MIN_IOU = 0.0\n\n    # NUM_GT x NUM_PRED\n    iou_matrix = np.zeros((n_true, n_pred))\n    for i in range(n_true):\n        for j in range(n_pred):\n            iou_matrix[i, j] = bbox_iou(bbox_gt[i,1:], bbox_pred[j,1:])\n\n    # call the Hungarian matching\n    idxs_true, idxs_pred = scipy.optimize.linear_sum_assignment(1 - iou_matrix)\n\n    if (not idxs_true.size) or (not idxs_pred.size):\n        ious = np.array([])\n    else:\n        ious = iou_matrix[idxs_true, idxs_pred]\n\n    ious_actual = iou_matrix[idxs_true, idxs_pred]\n    sel_valid = (ious_actual > IOU_THRESH)\n    label = sel_valid.astype(int)\n\n    return idxs_true, idxs_pred, ious_actual, label ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.853855Z","iopub.execute_input":"2024-01-01T17:42:22.854141Z","iopub.status.idle":"2024-01-01T17:42:22.863878Z","shell.execute_reply.started":"2024-01-01T17:42:22.854101Z","shell.execute_reply":"2024-01-01T17:42:22.863063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(test_img_list, prediction_path, label_path, IOU_THRESH=0.5): \n    tot_true_positive = 0\n    tot_false_positive = 0\n    tot_false_negative = 0\n    \n    precision = 0\n    recall = 0\n    f1 = 0    \n    \n    # accede alle singole immagini sulle quali è stat fatta predizione\n    for elem in test_img_list:\n        \n        true_positive = 0\n        false_positive = 0\n        false_negative = 0\n        \n        elem = elem.split('.')[0]\n        elem = elem + '.txt'\n        pred_img_path = os.path.join(prediction_path, elem)\n        label_img_path = os.path.join(label_path, elem)\n        \n        if not os.path.exists(pred_img_path):     # se non esite il file delle predizioni, tutti i box della groud truth saranno falsi negativi\n            with open(label_img_path, 'r') as file:\n                false_negative = len(file.readlines())\n                #print(f'TP:{true_positive}, FP:{false_positive}, FN:{false_negative}')\n                \n        else:   # se essite il file delle predizioni allora bisogna confrontare i box con quelli della ground truth\n            with open(pred_img_path, 'r') as boxfile:\n                boxes_pred = [line.strip() for line in boxfile]\n                boxes_pred = [list(map(float, stringa.split(' '))) for stringa in boxes_pred]\n                boxes_pred = np.vstack(boxes_pred)\n                \n            with open(label_img_path, 'r') as boxfile:\n                boxes_label = [line.strip() for line in boxfile]\n                boxes_label = [list(map(float, stringa.split(','))) for stringa in boxes_label]\n                if len(boxes_label) > 0:\n                    boxes_label = np.vstack(boxes_label)\n            \n            change_coordinate_format(boxes_pred)\n            change_coordinate_format(boxes_label)\n\n            match_result = match_bboxes(boxes_label, boxes_pred, IOU_THRESH)   #treshold è il valore di iou oltre il quale c'è un match tra i box (di default è 0.7)\n            #print(f'{match_result}\\n')\n            \n            # calcolo falsi negativi e falsi positivi\n            if boxes_label.shape[0] > boxes_pred.shape[0]:     # ho più box reali che predetti\n                false_negative = boxes_label.shape[0] - boxes_pred.shape[0]\n            elif boxes_label.shape[0] < boxes_pred.shape[0]:   # ho più box predetti che reali\n                false_positive = boxes_pred.shape[0] - boxes_label.shape[0]\n            \n            for i in range(len(match_result[0])):\n                #print(f'gt:{match_result[0][i]}, pred:{match_result[1][i]}, iou:{match_result[2][i]}, label:{match_result[3][i]}')\n            \n                index_gt = match_result[0][i]\n                index_pred = match_result[1][i]\n                if (boxes_label[index_gt,0] == boxes_pred[index_pred,0]) and (match_result[3][i] == 1):\n                    true_positive += 1\n                else:\n                    false_positive += 1\n                    false_negative += 1\n\n            #print(f'TP:{true_positive}, FP:{false_positive}, FN:{false_negative}')\n       \n        # calcolo metriche\n        precision += calculate_precision(true_positive, false_positive)\n        recall += calculate_recall(true_positive, false_negative)\n        f1 += calculate_f1_score(calculate_precision(true_positive, false_positive), calculate_recall(true_positive, false_negative))\n    \n        tot_true_positive += true_positive\n        tot_false_positive += false_positive\n        tot_false_negative += false_negative\n        \n    precision = precision/len(test_img_list)\n    recall = recall/len(test_img_list)\n    f1 = calculate_f1_score(precision, recall)\n    f1 = f1/len(test_img_list)\n    \n    print(f'precision:{precision}, recall:{recall}, f1_score:{f1}')\n            ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:35:09.871197Z","iopub.execute_input":"2024-01-01T18:35:09.871588Z","iopub.status.idle":"2024-01-01T18:35:09.887318Z","shell.execute_reply.started":"2024-01-01T18:35:09.871560Z","shell.execute_reply":"2024-01-01T18:35:09.886431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_coordinate_format(coordinate_list):\n    for i,coordinate in enumerate(coordinate_list):\n        x_center = coordinate[1]\n        y_center = coordinate[2]\n        width = coordinate[3]\n        heigth = coordinate[4]\n        coordinate[1] = x_center - width/2    # x_min\n        coordinate[2] = y_center - heigth/2   # y_min\n        coordinate[3] = x_center + width/2    # x_max\n        coordinate[4] = y_center + heigth/2   # y_max","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.882073Z","iopub.execute_input":"2024-01-01T17:42:22.882418Z","iopub.status.idle":"2024-01-01T17:42:22.892942Z","shell.execute_reply.started":"2024-01-01T17:42:22.882367Z","shell.execute_reply":"2024-01-01T17:42:22.892125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_labels(test_dataset, label_path, test_img_list):\n    labels_list = [test_dataset_elem[1] for test_dataset_elem in test_dataset]\n\n    for i, entry in enumerate(labels_list):\n        result_list = []\n        labels = entry['labels'].cpu().numpy()\n        boxes = entry['boxes'].cpu().numpy()\n        for k in range(boxes.shape[0]):\n            xmin, ymin, xmax, ymax = boxes[k]\n            x_centro = (xmin + xmax) / 2\n            y_centro = (ymin + ymax) / 2\n            width = xmax - xmin\n            height = ymax - ymin\n            box_data = np.array([labels[k], x_centro, y_centro, width, height])\n            result_list.append(box_data)\n            \n        np.savetxt(label_path + test_img_list[i].split('.')[0] + '.txt', result_list, fmt='%.6f', delimiter=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.894227Z","iopub.execute_input":"2024-01-01T17:42:22.894613Z","iopub.status.idle":"2024-01-01T17:42:22.906176Z","shell.execute_reply.started":"2024-01-01T17:42:22.894574Z","shell.execute_reply":"2024-01-01T17:42:22.905426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_labels(output, soglia_score, soglia_intersezione):   \n \n    output_tagliato = []\n\n    for detection in output:\n        boxes = detection['boxes']\n        labels = detection['labels']\n        scores = detection['scores']\n\n        # Trova gli indici dei box che superano la soglia di score\n        indici_superati_soglia = scores >= soglia_score\n\n        # Filtra i box, le label e gli score in base agli indici superati la soglia\n        boxes_tagliati = boxes[indici_superati_soglia]\n        labels_tagliati = labels[indici_superati_soglia]\n        scores_tagliati = scores[indici_superati_soglia]\n\n        # Ordina i box in base allo score in ordine decrescente\n        indici_ordine_decrescente = torch.argsort(scores_tagliati, descending=True)\n        boxes_tagliati = boxes_tagliati[indici_ordine_decrescente]\n        labels_tagliati = labels_tagliati[indici_ordine_decrescente]\n        scores_tagliati = scores_tagliati[indici_ordine_decrescente]\n\n        # Calcola l'area di intersezione tra tutti i box rimasti\n        area_intersezione = torch.zeros(len(scores_tagliati))\n        for i in range(len(scores_tagliati)):\n            for j in range(i + 1, len(scores_tagliati)):\n                # Calcola l'area di intersezione\n                x_min = max(boxes_tagliati[i, 0], boxes_tagliati[j, 0])\n                y_min = max(boxes_tagliati[i, 1], boxes_tagliati[j, 1])\n                x_max = min(boxes_tagliati[i, 2], boxes_tagliati[j, 2])\n                y_max = min(boxes_tagliati[i, 3], boxes_tagliati[j, 3])\n                area_intersezione[i] = max(0, x_max - x_min) * max(0, y_max - y_min)\n\n        # Trova gli indici dei box con area di intersezione vicina a 1 e tiene solo il box con lo score maggiore\n        indici_da_tenere = torch.ones(len(scores_tagliati), dtype=torch.bool)\n        for i in range(len(scores_tagliati)):\n            for j in range(i + 1, len(scores_tagliati)):\n                if area_intersezione[i] / min(area_intersezione[i], area_intersezione[j]) > soglia_intersezione:\n                    # Elimina il box con area di intersezione vicina a 1 se ha uno score inferiore\n                    if scores_tagliati[i] <= scores_tagliati[j]:\n                        indici_da_tenere[i] = False\n                    else:\n                        indici_da_tenere[j] = False\n\n        # Filtra i box, le label e gli score in base agli indici da tenere\n        boxes_tagliati = boxes_tagliati[indici_da_tenere]\n        labels_tagliati = labels_tagliati[indici_da_tenere]\n        scores_tagliati = scores_tagliati[indici_da_tenere]\n\n        # Creare un nuovo dizionario con i risultati tagliati\n        detection_tagliata = {\n            'boxes': boxes_tagliati,\n            'labels': labels_tagliati,\n            'scores': scores_tagliati\n        }\n\n        # Aggiungere il risultato alla lista finale solo se ci sono box sopra la soglia\n        if len(boxes_tagliati) > 0:\n            output_tagliato.append(detection_tagliata)\n            \n    return output_tagliato","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.907589Z","iopub.execute_input":"2024-01-01T17:42:22.907906Z","iopub.status.idle":"2024-01-01T17:42:22.920224Z","shell.execute_reply.started":"2024-01-01T17:42:22.907876Z","shell.execute_reply":"2024-01-01T17:42:22.919305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_crop(original_width, original_heigth, scarto_sovrapposizione=200):\n    #Definizione lista di coordinate per i crop delle immagini (x)\n    dim_x = original_width\n    x_init = 0\n    x_end = 640\n    lista_x = []\n\n    while (x_end <= dim_x):\n        lista_x.append([x_init, x_end])\n        x_init = x_end - scarto_sovrapposizione\n        x_end = x_init + 640\n\n    if (x_end > dim_x):\n        lista_x.append([x_init, dim_x])\n\n    #Definizione lista di coordinate per i crop delle immagini (y)\n    dim_y = original_heigth\n    y_init = 0\n    y_end = 640\n    scarto_sovrapposizione = 200\n    lista_y = []\n\n    while (y_end <= dim_y):\n        lista_y.append([y_init, y_end])\n        y_init = y_end - scarto_sovrapposizione\n        y_end = y_init + 640\n\n    if (y_end > dim_y):\n        lista_y.append([y_init, dim_y])\n    \n    return lista_x, lista_y","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.921236Z","iopub.execute_input":"2024-01-01T17:42:22.921509Z","iopub.status.idle":"2024-01-01T17:42:22.934315Z","shell.execute_reply.started":"2024-01-01T17:42:22.921487Z","shell.execute_reply":"2024-01-01T17:42:22.933446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_crop(img, lista_x, lista_y, crop_img_path):\n    lista_img = []\n\n    for elem_y in lista_y:\n        for elem_x in lista_x:\n            dim_x = elem_x[1] - elem_x[0]\n            dim_y = elem_y[1] - elem_y[0]\n            if (dim_x == 640) and (dim_y == 640) :\n                lista_img.append(img[elem_y[0]:elem_y[1], elem_x[0]:elem_x[1], :])\n            else:\n                temp = img[elem_y[0]:elem_y[1], elem_x[0]:elem_x[1], :]\n                if (dim_x != 640):\n                    padding = np.zeros((dim_y, 640-dim_x, 3), dtype=np.uint8)\n                    temp = np.concatenate((temp,padding), axis=1)     #concatena l'immagine temp e gli zeri lungo la direzione orizzontale\n                    dim_x = 640\n                if (dim_y != 640):\n                    padding = np.zeros((640-dim_y, dim_x, 3), dtype=np.uint8)\n                    temp = np.concatenate((temp,padding), axis=0)     #concatena l'immagine temp e gli zeri lungo la direzione verticale\n                    dim_y = 640\n                lista_img.append(temp)\n                if(temp.shape != (640,640,3)):\n                    print('sceeeeem')\n\n    # Save each image in lista_img\n    for i, img_chunk in enumerate(lista_img):\n        save_path = crop_img_path + 'image_' + str(i) + '.jpg'\n        plt.imsave(save_path, img_chunk)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.935538Z","iopub.execute_input":"2024-01-01T17:42:22.935877Z","iopub.status.idle":"2024-01-01T17:42:22.946459Z","shell.execute_reply.started":"2024-01-01T17:42:22.935848Z","shell.execute_reply":"2024-01-01T17:42:22.945706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregate_label(crop_img_path, crop_label_path, output_file_path, posizioni, original_width, original_height):\n    for elem in os.listdir(crop_label_path):\n        img_name = elem.split('.txt')[0]\n        img_path = crop_img_path + img_name + '.jpg'\n        img = plt.imread(img_path)\n        heigth, width, band = img.shape\n \n        with open(crop_label_path + elem, 'r') as file:\n            boxes_label = [line.strip() for line in file]\n            boxes_label = [list(map(float, stringa.split(','))) for stringa in boxes_label] \n            index = int(img_name.split('_')[-1])\n \n            with open(output_file_path, 'a') as file:\n                for line in boxes_label:\n                    # Sposta le coordinate senza normalizzarle\n                    x_center = line[1] + posizioni[index][0]\n                    y_center = line[2] + posizioni[index][1]\n                    box_width = line[3]\n                    box_height = line[4]\n \n                    # Scrivi la riga nel formato desiderato\n                    file.write(f\"{line[0]} {x_center} {y_center} {box_width} {box_height} {index}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.947470Z","iopub.execute_input":"2024-01-01T17:42:22.947792Z","iopub.status.idle":"2024-01-01T17:42:22.960381Z","shell.execute_reply.started":"2024-01-01T17:42:22.947760Z","shell.execute_reply":"2024-01-01T17:42:22.959628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_overlapped_boxes(label_path, treshold=0.25):\n    indici_da_eliminare = []\n\n    with open (label_path, 'r') as file:\n        boxes = [line.strip() for line in file]\n        boxes = [list(map(float, stringa.split())) for stringa in boxes]\n        \n        #print(boxes[0])\n\n    # boxes_temp contiene i box con le coordinate convertite nel formato: (label,xmin,ymin,xmax,ymax)\n    boxes_temp = copy.deepcopy(boxes)\n    change_coordinate_format(boxes_temp)\n    \n\n    # alcuni box si sovrappongono perché i crop sono con sovrapposizione, quindi applichiamo una sorta di non maximal suppression\n    # confrontiamo ciascun box con tutti gli altri della llista (escluso se stesso)\n    # se due box sono della stessa classe e il valore di sovrapposizione supera una certa soglia (calcolata con iou) allora il più piccolo dei due viene eliminato\n    for i,box in tqdm(enumerate(boxes_temp)):\n        for j,box_2 in enumerate(boxes_temp):\n            if j != i:\n                #print(f'box {box}')\n                #print(f'box_2 {box_2}')\n\n                # confrontiamo i box da eliminare solo se provengono da crop diversi (index aggiunto prima), per eliminare solo i box dovuti aalle sovrapposizioni dei crop (uso un basso valore di treshold)\n                if box[5] != box_2[5]:      \n                    \n                    if box[0] == box_2[0]:\n                        iou = bbox_iou(box[1:], box_2[1:])\n                        if iou >= treshold:\n                            area = (box[3] - box[1]) * (box[4] - box[2])    #area = (xmax-xmin)*(ymax-ymin)\n                            area_2 = (box_2[3] - box_2[1]) * (box_2[4] - box_2[2])\n                            if area_2 <= area:\n                                indici_da_eliminare.append(j)\n                                \n                else:\n                    # cerco di eliminare solo i box fortemente sovrapposti (treshold alto) dovuti ad una detection poco precisa del modello \n                    pass\n\n    # la lista di indici è ordinata in modo decrescente, per evitare che durante l'eliminazione ci siano incongruenze tra gli indici\n    # (eliminando in ordine crescente, il valore degli indici va a scalare e quindi non si trovano più)\n    indici_da_eliminare = list(set(indici_da_eliminare))\n    indici_da_eliminare.sort(reverse=True)\n    for elem in indici_da_eliminare:\n        del boxes[elem]\n\n    # riscriviamo il file dei box,che conterrà ora solo i box non sovrapposti\n    with open(label_path, 'w') as file:\n        for line in boxes:\n            #print(f\"{line[0]} {line[1]} {line[2]} {line[3]} {line[4]}\\n\")\n            file.write(f\"{line[0]} {line[1]} {line[2]} {line[3]} {line[4]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.961346Z","iopub.execute_input":"2024-01-01T17:42:22.961626Z","iopub.status.idle":"2024-01-01T17:42:22.975539Z","shell.execute_reply.started":"2024-01-01T17:42:22.961597Z","shell.execute_reply":"2024-01-01T17:42:22.974642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(crop, crop_pred_path, crop_elem):\n    model.eval()\n    test_image = crop[0].to(device)\n    test_image = test_image.unsqueeze(0)\n    \n    with torch.no_grad():\n        output = model(test_image)  \n        \n    soglia_score = 0.1\n    soglia_intersezione = 0.9  # Puoi regolare questa soglia in base alle tue esigenze\n    temp = remove_labels(output, soglia_score, soglia_intersezione)\n\n    # Cambio di formato in classe, x_cen, y_cen, width, height\n    result_list = []\n    for entry in temp:\n        labels = entry['labels'].cpu().numpy()\n        boxes = entry['boxes'].cpu().numpy()\n        for k in range(boxes.shape[0]):\n            xmin, ymin, xmax, ymax = boxes[k]\n            x_centro = (xmin + xmax) / 2\n            y_centro = (ymin + ymax) / 2\n            width = xmax - xmin\n            height = ymax - ymin\n            box_data = np.array([labels[k], x_centro, y_centro, width, height])\n            result_list.append(box_data)\n\n    #print(result_list)\n\n    # Salvataggio su file txt\n    np.savetxt(crop_pred_path + crop_elem.split('.')[0] + '.txt', result_list, fmt='%.6f', delimiter=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.976674Z","iopub.execute_input":"2024-01-01T17:42:22.977351Z","iopub.status.idle":"2024-01-01T17:42:22.990312Z","shell.execute_reply.started":"2024-01-01T17:42:22.977320Z","shell.execute_reply":"2024-01-01T17:42:22.989559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_with_boxes(image_path, annotation_file):\n    # Carica l'immagine come array NumPy\n    image_array = imageio.v2.imread(image_path)\n    \n    # Converti l'array NumPy in un oggetto Image\n    image = Image.fromarray(image_array)\n    \n    # Crea un oggetto ImageDraw per disegnare i bounding box sull'immagine\n    draw = ImageDraw.Draw(image)\n    \n    # Dizionario che mappa classi a colori\n    class_colors = {\n        48: \"yellow\",\n        5: \"green\",\n    }\n    default_color = 'red'\n\n    # Leggi le annotazioni dal file di testo\n    with open(annotation_file, 'r') as f:\n        for line in f:\n            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n            \n            # Calcola le coordinate del bounding box in formato assoluto\n            image_width, image_height = image.size\n            x1 = int(x_center - width / 2)\n            y1 = int(y_center - height / 2)\n            x2 = int(x_center + width / 2)\n            y2 = int(y_center + height / 2)\n            color = class_colors.get(int(class_id), default_color)\n\n            # Disegna il bounding box sull'immagine\n            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n            draw.text((x1, y1), f\"Class: {int(class_id)}\", fill=color)\n\n    # Mostra l'immagine con i bounding box\n    plt.figure()\n    plt.imshow(image)\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:22.991271Z","iopub.execute_input":"2024-01-01T17:42:22.991572Z","iopub.status.idle":"2024-01-01T17:42:23.004206Z","shell.execute_reply.started":"2024-01-01T17:42:22.991536Z","shell.execute_reply":"2024-01-01T17:42:23.003299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Def. del Modello","metadata":{}},{"cell_type":"code","source":"# Crea il modello Faster R-CNN\nmodel_name = 'resnet50'\nnum_classes = 60\nmodel = get_model(model_name, num_classes)\n\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:23.005326Z","iopub.execute_input":"2024-01-01T17:42:23.005688Z","iopub.status.idle":"2024-01-01T17:42:24.367903Z","shell.execute_reply.started":"2024-01-01T17:42:23.005656Z","shell.execute_reply":"2024-01-01T17:42:24.367130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurazione della trasformazione per l'oggetto model\nmodel.transform = GeneralizedRCNNTransform(\n    min_size=(640,),             # Dimensione minima dell'immagine durante la trasformazione\n    max_size=640,                 # Dimensione massima dell'immagine durante la trasformazione\n    image_mean=[0.485, 0.456, 0.406],   # Media dell'immagine per la normalizzazione\n    image_std=[0.229, 0.224, 0.225]     # Deviazione standard dell'immagine per la normalizzazione\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:24.368981Z","iopub.execute_input":"2024-01-01T17:42:24.369264Z","iopub.status.idle":"2024-01-01T17:42:24.374377Z","shell.execute_reply.started":"2024-01-01T17:42:24.369240Z","shell.execute_reply":"2024-01-01T17:42:24.373436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sposta il modello sulla GPU, se disponibile\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:24.375509Z","iopub.execute_input":"2024-01-01T17:42:24.375795Z","iopub.status.idle":"2024-01-01T17:42:29.357403Z","shell.execute_reply.started":"2024-01-01T17:42:24.375772Z","shell.execute_reply":"2024-01-01T17:42:29.356612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carichiamo i pesi relativi al modello migliore\npesi = '/kaggle/input/dataset-ipcv-teama/best_model_weights_L2rpn_L2roi_10epoch_16batch_0.005lr.pth'\nif not torch.cuda.is_available():\n    weights = torch.load(pesi, map_location=torch.device('cpu'))\nelse:\n    weights = torch.load(pesi)\nmodel.load_state_dict(weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:29.358654Z","iopub.execute_input":"2024-01-01T17:42:29.359042Z","iopub.status.idle":"2024-01-01T17:42:30.707793Z","shell.execute_reply.started":"2024-01-01T17:42:29.359009Z","shell.execute_reply":"2024-01-01T17:42:30.706289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING \n## Test Locale: faccio predizione sui crop e li rimetto insieme per ottenere la predizione sull'intera immagine, poi calcolo le metriche","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:32:50.529369Z","iopub.execute_input":"2023-12-30T14:32:50.529609Z","iopub.status.idle":"2023-12-30T14:32:56.546441Z","shell.execute_reply.started":"2023-12-30T14:32:50.529587Z","shell.execute_reply":"2023-12-30T14:32:56.545624Z"}}},{"cell_type":"code","source":"test_dataset = BuildingsDataset(test_path, coco_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:30.708849Z","iopub.execute_input":"2024-01-01T17:42:30.709129Z","iopub.status.idle":"2024-01-01T17:42:32.933616Z","shell.execute_reply.started":"2024-01-01T17:42:30.709105Z","shell.execute_reply":"2024-01-01T17:42:32.932649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(output_file_path):\n    os.mkdir(output_file_path)\n\nif not os.path.exists(local_crop_img_path):\n    os.mkdir(local_crop_img_path)\n\ntest_img_list = sorted(os.listdir(test_path))\nfor i,line in enumerate(test_img_list):\n    test_img_list[i] = line.split('/')[-1]    # prendo il nome dell'immagine con l'estensione .jpg\n\nfor i, elem in enumerate(test_img_list):\n    if not os.path.exists(crop_pred_path):\n        os.mkdir(crop_pred_path)    \n    \n    # apriamo l'immagine per ottenere la lunghezza e la larghezza\n    img = plt.imread(test_path + '/' + elem)\n    original_heigth, original_width, _ = img.shape\n    lista_x, lista_y = calculate_crop(original_width, original_heigth, scarto_sovrapposizione=200)\n    \n    # creazione dei crop per la singola immagine\n    crop_save_path = local_crop_img_path + test_img_list[i].split('.')[0] + '/'\n    if not os.path.exists(crop_save_path):\n        os.mkdir(crop_save_path)\n        create_crop(img, lista_x, lista_y, crop_save_path)\n    \n    test_crop_dataset = BuildingsDataset(crop_save_path)\n    crop_list = sorted(os.listdir(crop_save_path))\n\n    for j, crop in enumerate(test_crop_dataset):\n        make_predictions(crop, crop_pred_path, crop_list[j])\n    \n    # viene creato un vettore di posizioni, che tiene conto della posizione di ciascun crop rispetto all'immagine originale (as es. (0,0), (0,640), (640,640), ...)\n    # servono per adattare (ovvero spostare) i box calcolati sui crop all'immagine originale (quella grande)\n    posizioni = []\n    for y,elem_y in enumerate(lista_y):\n        for elem_x in lista_x:\n            posizioni.append((elem_x[0],elem_y[0]))\n                \n    # i box dei diversi crop vengono uniti in un unico file, questi box vengono anche adattati all'immagine originale tramite shift e normalizzazione\n    test_label_path = output_file_path + elem.split('.')[0] + '.txt'\n    aggregate_label(crop_save_path, crop_pred_path, test_label_path, posizioni, original_width, original_heigth)\n\n    if os.path.exists(test_label_path):\n        # vengono rimossi i box sovrapposti\n        remove_overlapped_boxes(test_label_path, treshold=0.25)\n    \n    shutil.rmtree(crop_pred_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:32.935007Z","iopub.execute_input":"2024-01-01T17:42:32.935379Z","iopub.status.idle":"2024-01-01T18:25:11.394535Z","shell.execute_reply.started":"2024-01-01T17:42:32.935343Z","shell.execute_reply":"2024-01-01T18:25:11.393653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(label_path):\n    os.mkdir(label_path)\nsave_labels(test_dataset, label_path, test_img_list)\ncalculate_metrics(test_img_list, output_file_path, label_path, IOU_THRESH=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:25:11.395811Z","iopub.execute_input":"2024-01-01T18:25:11.396097Z","iopub.status.idle":"2024-01-01T18:29:36.347853Z","shell.execute_reply.started":"2024-01-01T18:25:11.396072Z","shell.execute_reply":"2024-01-01T18:29:36.346787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_list_tot = sorted(os.listdir(test_path))\ntest_img_list = []\ntest_img_list.append(test_img_list_tot[75])\ntest_img_list.append(test_img_list_tot[63])\nfor i, elem in enumerate(test_img_list):\n    image_final = show_image_with_boxes(test_path + '/' + elem, '/kaggle/working/local_test_predictions/' + elem.split('.')[0] + '.txt')  \n    image_final.save(path_out + 'risultati_' + elem.split('.')[0] + '.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:29:36.353809Z","iopub.execute_input":"2024-01-01T18:29:36.354101Z","iopub.status.idle":"2024-01-01T18:29:41.321395Z","shell.execute_reply.started":"2024-01-01T18:29:36.354075Z","shell.execute_reply":"2024-01-01T18:29:41.320503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING - Predizioni sul test set (set di immagini non labellato)","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(nolabels_output_file_path):\n    os.mkdir(nolabels_output_file_path)\n\nif not os.path.exists(crop_img_path):\n    os.mkdir(crop_img_path)\n\ntest_img_list_tot = sorted(os.listdir(nolabels_test_path))\ntest_img_list = []\ntest_img_list.append(test_img_list_tot[75])\ntest_img_list.append(test_img_list_tot[63])\ntest_img_list.append(test_img_list_tot[101])\ntest_list = test_img_list.copy()\n\nfor i,line in enumerate(test_img_list):\n    test_img_list[i] = line.split('/')[-1]    # prendo il nome dell'immagine con l'estensione .jpg\n\nfor i, elem in enumerate(test_img_list):\n    if not os.path.exists(crop_pred_path):\n        os.mkdir(crop_pred_path)    \n    \n    # apriamo l'immagine per ottenere la lunghezza e la larghezza\n    img = plt.imread(nolabels_test_path + '/' + elem)\n    original_heigth, original_width, _ = img.shape\n    lista_x, lista_y = calculate_crop(original_width, original_heigth, scarto_sovrapposizione=200)\n    \n    # creazione dei crop per la singola immagine\n    crop_save_path = crop_img_path + test_img_list[i].split('.')[0] + '/'\n    if not os.path.exists(crop_save_path):\n        os.mkdir(crop_save_path)\n        create_crop(img, lista_x, lista_y, crop_save_path)\n    \n    test_crop_dataset = BuildingsDataset(crop_save_path)\n    crop_list = sorted(os.listdir(crop_save_path))\n\n    for j, crop in enumerate(test_crop_dataset):\n        make_predictions(crop, crop_pred_path, crop_list[j])\n    \n    # viene creato un vettore di posizioni, che tiene conto della posizione di ciascun crop rispetto all'immagine originale (as es. (0,0), (0,640), (640,640), ...)\n    # servono per adattare (ovvero spostare) i box calcolati sui crop all'immagine originale (quella grande)\n    posizioni = []\n    for y,elem_y in enumerate(lista_y):\n        for elem_x in lista_x:\n            posizioni.append((elem_x[0],elem_y[0]))\n                \n    # i box dei diversi crop vengono uniti in un unico file, questi box vengono anche adattati all'immagine originale tramite shift e normalizzazione\n    test_label_path = nolabels_output_file_path + elem.split('.')[0] + '.txt'\n    aggregate_label(crop_save_path, crop_pred_path, test_label_path, posizioni, original_width, original_heigth)\n\n    if os.path.exists(test_label_path):\n        # vengono rimossi i box sovrapposti\n        remove_overlapped_boxes(test_label_path, treshold=0.25)\n    \n    shutil.rmtree(crop_pred_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:29:41.322544Z","iopub.execute_input":"2024-01-01T18:29:41.322832Z","iopub.status.idle":"2024-01-01T18:32:31.024256Z","shell.execute_reply.started":"2024-01-01T18:29:41.322808Z","shell.execute_reply":"2024-01-01T18:32:31.023344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, elem in enumerate(test_img_list):\n    image_final = show_image_with_boxes(nolabels_test_path + '/' + elem, '/kaggle/working/test_predictions/' + elem.split('.')[0] + '.txt')  \n    image_final.save(path_out + 'risultati_' + elem.split('.')[0] + '.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:32:31.025475Z","iopub.execute_input":"2024-01-01T18:32:31.025771Z","iopub.status.idle":"2024-01-01T18:32:37.651770Z","shell.execute_reply.started":"2024-01-01T18:32:31.025746Z","shell.execute_reply":"2024-01-01T18:32:37.650805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confronto con predizioni su Img intere","metadata":{}},{"cell_type":"code","source":"def visualize_results_with_boxes(image_path, results):\n    bounding_boxes = results['boxes']\n\n    # Carica l'immagine usando PIL\n    image = Image.open(image_path)\n\n    # Inizializza ImageDraw con l'immagine\n    draw = ImageDraw.Draw(image)\n\n    # Disegna i bounding box sull'immagine\n    for bbox in bounding_boxes:\n        bbox_coords = bbox  # Converti la stringa bbox in una lista di coordinate\n        bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[2],bbox_coords[3]]\n        draw.rectangle(bbox_coords, outline='red', width=2)\n\n    # Visualizza l'immagine con i bounding box\n    plt.imshow(image)\n    #plt.show()\n    return image\n\ndef visualize_image_with_boxes(image_path, coco_annotations_path):\n    # Carica il file COCO annotations JSON\n    with open(coco_annotations_path, 'r') as coco_file:\n        coco_data = json.load(coco_file)\n\n    # Estrai l'id dell'immagine dal nome del file\n    image_name = image_path.split(\"/\")[-1]\n    image_id = next((img['id'] for img in coco_data['images'] if img['file_name'] == image_name), None)\n\n    # Se l'id dell'immagine è trovato, estrai i bounding box corrispondenti\n    if image_id is not None:\n        bounding_boxes = [bbox for bbox in coco_data['annotations'] if bbox['image_id'] == image_id]\n        \n        # Carica l'immagine usando PIL\n        image = Image.open(image_path)\n\n        # Inizializza ImageDraw con l'immagine\n        draw = ImageDraw.Draw(image)\n\n        # Disegna i bounding box sull'immagine\n        for bbox in bounding_boxes:\n            bbox_coords = eval(bbox['bbox'])  # Converti la stringa bbox in una lista di coordinate\n            bbox_coords = [bbox_coords[0],bbox_coords[1],bbox_coords[0]+bbox_coords[2],bbox_coords[1]+bbox_coords[3]]\n            draw.rectangle(bbox_coords, outline='red', width=2)\n\n        # Visualizza l'immagine con i bounding box\n        plt.imshow(image)\n        #plt.show()\n        return image\n    else:\n        print(f\"Image {image_name} not found in COCO annotations.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:32:37.652872Z","iopub.execute_input":"2024-01-01T18:32:37.653156Z","iopub.status.idle":"2024-01-01T18:32:37.665728Z","shell.execute_reply.started":"2024-01-01T18:32:37.653131Z","shell.execute_reply":"2024-01-01T18:32:37.664808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## test locale (con labels)","metadata":{}},{"cell_type":"code","source":"test_images = sorted(os.listdir(test_path))\nindici = [75,63]\n\nfor indice in indici:\n    # Test su un'immagine\n    model.eval()\n    test_image = test_dataset[indice][0]\n\n    test_image = test_image.to(device)\n    test_image = test_image.unsqueeze(0)\n    with torch.no_grad():\n        output = model(test_image) \n\n    soglia_score = 0.1\n    soglia_intersezione = 0.85  # Puoi regolare questa soglia in base alle tue esigenze\n\n    output_tagliato = remove_labels(output, soglia_score, soglia_intersezione)\n    elem = test_images[indice]\n\n    original_image_path = test_path + '/' + elem\n    plt.figure(figsize=(10,10))    \n    img = visualize_results_with_boxes(original_image_path, output_tagliato[0])\n    img.save(path_out + 'pred_intera_' + elem.split('.')[0] + '.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:32:37.666762Z","iopub.execute_input":"2024-01-01T18:32:37.667048Z","iopub.status.idle":"2024-01-01T18:32:45.447892Z","shell.execute_reply.started":"2024-01-01T18:32:37.667024Z","shell.execute_reply":"2024-01-01T18:32:45.446981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## test senza labels","metadata":{}},{"cell_type":"code","source":"test_images = sorted(os.listdir(nolabels_test_path))\nindici = [75,63,101]\n\nnolabels_test_dataset = BuildingsDataset(nolabels_test_path)\nfor indice in indici:\n    # Test su un'immagine\n    model.eval()\n    test_image = nolabels_test_dataset[indice][0]\n\n    test_image = test_image.to(device)\n    test_image = test_image.unsqueeze(0)\n    with torch.no_grad():\n        output = model(test_image) \n\n    soglia_score = 0.1\n    soglia_intersezione = 0.85  # Puoi regolare questa soglia in base alle tue esigenze\n\n    output_tagliato = remove_labels(output, soglia_score, soglia_intersezione)\n    elem = test_images[indice]\n\n    original_image_path = nolabels_test_path + '/' + elem\n    plt.figure(figsize=(10,10))    \n    img = visualize_results_with_boxes(original_image_path, output_tagliato[0])\n    img.save(path_out + 'pred_intera_' + elem.split('.')[0] + '.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:32:45.449038Z","iopub.execute_input":"2024-01-01T18:32:45.449369Z","iopub.status.idle":"2024-01-01T18:32:57.240932Z","shell.execute_reply.started":"2024-01-01T18:32:45.449342Z","shell.execute_reply":"2024-01-01T18:32:57.240060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}